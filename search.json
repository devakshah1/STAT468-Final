[
  {
    "objectID": "final_project_report.html",
    "href": "final_project_report.html",
    "title": "STAT468 Final Project",
    "section": "",
    "text": "The objective of this project is to investigate the relationship between OHL performance / player build and NHL career longevity. We will analyze how various factors such as goals per game, assists per game, height, and weight influence the probability of a player playing more than 200 games in the NHL.\nCHAPTER 1: IMPORT\nThe first step in our project is to import the necessary libraries and data:\n\nimport TopDownHockey_Scraper.TopDownHockey_NHL_Scraper as tdhnhlscrape\nimport TopDownHockey_Scraper.TopDownHockey_EliteProspects_Scraper as tdhepscrape\nfrom nhlpy import NHLClient\n\n\nimport pandas as pd\nfrom datetime import date\nimport statsmodels.api as sm\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom lets_plot import *\nLetsPlot.setup_html()\nfrom pins import board_s3\nfrom vetiver import vetiver_pin_write\nfrom vetiver import VetiverModel\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nfrom vetiver.handlers.base import BaseHandler\nimport statsmodels.api as sm\n\n\n            \n            \n            \n\n\nWe will create a toggle to see if we want to rescrape the data or just import the prior scrape from an existing file. If scrape is set to True, we will scrape the data; otherwise, we will read from the existing file.\n\nscrape = False # Set to True to scrape data, False to use existing data\n\n#This will be the file name where data is stored in our git repo\nfilename = \"regression_input.xlsx\"\n\nFirst, we loop through a bunch of seasons and scrape player OHL stats and builds. Note that we get rid of player positions so that we can join with player builds using names as index. While this is mixing some of steps 2&3 (Tidy&Transform), it’s easier to do it here since we want to get the player builds at the time of the draft, and it would be confusing to do the join later on, when we’re outside the loop and the stats&builds may not correspond to one another since they could be from seperate years.\n\n#Get OHL Player Data (stats, build, etc)\n\nif scrape == True:\n\n    #Can't include all years because of backend API issues in some years\n    years = [\"2004-2005\", \"2006-2007\", \"2007-2008\", \"2008-2009\", \"2009-2010\", \"2010-2011\", \"2011-2012\", \"2012-2013\", \"2013-2014\", \n             \"2014-2015\", \"2015-2016\", \"2016-2017\", \"2017-2018\", \"2018-2019\"]\n\n    aggregated_output = pd.DataFrame()\n\n    for year in years:\n        df = tdhepscrape.get_skaters((\"ohl\"), (year))\n\n        #GET PLAYER INFO\n        info = tdhepscrape.get_player_information(df)\n\n        #GET RID OF DEFENCEMEN\n        df = df[~df['player'].str.contains(r'\\(([^)]*D[^)]*)\\)', regex=True)]\n\n        #GET RID OF PLAYER POSITIONS FROM NAMES\n        df['player'] = df['player'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n\n        #ADD YEAR TO DF\n        df.insert(0, \"year\", year)\n\n        #JOIN PLAYER BIO WITH STATS\n        year_output = pd.merge(df[[\"year\", \"player\", \"gp\", \"g\", \"a\", \"tp\"]], info[[\"player\", \"dob\", \"height\", \"weight\", \"shoots\"]], on='player', how='inner')\n\n        #ADD CURRENT YEAR PROSPECTS TO AGGREGATED DF\n        aggregated_output = pd.concat([aggregated_output, year_output])\nelse:\n    print(\"Scrape completed prior\")\n\nScrape completed prior\n\n\nNext, we print the draftyears of all NHL players drafted between 2005 and 2020\n\nif scrape == True:\n    years = list(range(2005, 2021))\n\n    draftyears = pd.DataFrame()\n\n    for year in years:\n        df_list = pd.read_html(f\"https://www.hockey-reference.com/draft/NHL_{year}_entry.html\", match=\"Round\")\n\n        players_drafted = df_list[0]\n\n        players_drafted\n\n        #Let's get rid of the top header that isnt really used\n        players_drafted.columns = players_drafted.columns.get_level_values(1)\n\n        players_drafted[\"draft_year\"] = year\n        players_drafted = players_drafted[[\"draft_year\", \"Player\"]]\n        players_drafted =players_drafted.rename(columns={\"draft_year\": \"player_draft_year\", \"Player\": \"player\"})\n        draftyears = pd.concat([draftyears, players_drafted])\n\n    print(draftyears)        \n\nFinally, we get the games played for all NHL players to have played at least 1 game in the NHL. We will use this to calculate the probability of a player playing more than 200 games in the NHL.\n\n#Getting games played for all NHL players\nif scrape == True:\n    nhl_gp = pd.DataFrame()\n    pages = list(range(1, 80))\n\n    for page in pages:\n        df_list = pd.read_html(f\"https://www.eliteprospects.com/league/nhl/stats/all-time?page={page}\")\n        page_stats = df_list[2]\n        page_stats = page_stats[[\"Player\", \"GP\"]]\n        nhl_gp = pd.concat([nhl_gp, page_stats])\n\n    nhl_gp = nhl_gp.rename(columns={\"Player\": \"player\", \"GP\": \"nhl_gp\"})\n\n    #GET RID OF PLAYER POSITIONS FROM NAMES\n    nhl_gp['player'] = nhl_gp['player'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n\n    # Replacing NA and \"-\" values with 0\n    nhl_gp[\"nhl_gp\"] = nhl_gp[\"nhl_gp\"].fillna(0)\n    nhl_gp[\"nhl_gp\"] = nhl_gp[\"nhl_gp\"].replace(\"-\", 0)\n\n    nhl_gp\n\nCHAPTER 2: TIDY\nNext, we change the type of some columns - they are objects by default, we need them to be integeres to regress on them later on. We also replace “-” values with 0 for regression purposes, since “-” means that the player did not play in that season, and we want to treat that as 0 games played.\n\nif scrape == True:\n    aggregated_output[\"gp\"] = aggregated_output[\"gp\"].replace(\"-\", 0)\n    aggregated_output[\"gp\"] = aggregated_output[\"gp\"].astype(int)\n    aggregated_output[\"g\"] = aggregated_output[\"g\"].replace(\"-\", 0)\n    aggregated_output[\"g\"] = aggregated_output[\"g\"].astype(int)\n    aggregated_output[\"a\"] = aggregated_output[\"a\"].replace(\"-\", 0)\n    aggregated_output[\"a\"] = aggregated_output[\"a\"].astype(int)\n    aggregated_output[\"tp\"] = aggregated_output[\"tp\"].replace(\"-\", 0)\n    aggregated_output[\"tp\"] = aggregated_output[\"tp\"].astype(int)\n\nGet corresponding draft year for each row in the dataset (oe. season 2024-2025 would have a draft year of 2025).\nNote this isn’t necessarily the players’ draft year - it’s just the draft year for the corresponding row’s season. We will use this to only keep rows where the draft year is the same as the player’s actual draft year.\n\nif scrape == True:\n    aggregated_output[\"draft_year\"] = aggregated_output[\"year\"].str[5:]\n    aggregated_output[\"draft_year\"] = aggregated_output[\"draft_year\"].astype(int)\n    aggregated_output\n\nJoin games played and draft year onto each row of the OHL player data dataset.\n\n#Join GP, draft year onto OHL player data\nif scrape == True:\n    #MAKE ALL PLAYER NAMES UPPERCASE (TO MAKE JOINING TABLES NON CASE SENSITIVE)\n    aggregated_output['player'] = aggregated_output['player'].str.upper()\n    draftyears['player'] = draftyears['player'].str.upper()\n    nhl_gp['player'] = nhl_gp['player'].str.upper()\n\n\n    #Filter ohl stats for only drafted players' draft year stats - \n    #This will get rid of a) undrafted players, and b) drafted players non-draft year stats\n    df = pd.merge(aggregated_output, draftyears, left_on=['player', 'draft_year'], right_on=['player', 'player_draft_year'], how='inner')\n\n    #Can get rid of one of the draft year columns - don't need both\n    df = df[[\"year\", \"player\", \"gp\", \"g\", \"a\", \"tp\", \"dob\", \"height\", \"weight\", \"shoots\", 'draft_year']]\n\n    #Join players' games played - if player gp not found, assume it to be 0.\n    df = pd.merge(df, nhl_gp, left_on=['player'], right_on=['player'], how='left')\n    df[\"nhl_gp\"] = df[\"nhl_gp\"].fillna(0)\n    df[\"nhl_gp\"] = df[\"nhl_gp\"].astype(int)\n\n    df.to_excel(filename, index=False)\n\nFinally, some miscallaneous tidying. Refer to comments for more information.\n\ndf = pd.read_excel(filename)\n\n#Changing the weight to a numerical variate in kg\ndf[\"weight_kg\"] = df[\"weight\"].apply(lambda x: x[:x.find(\" \")])\ndf[\"weight_kg\"] = df[\"weight_kg\"].astype(int)\ndf.dtypes\n\n#Can get rid of intermediate columns\ndf = df.drop([\"weight\"], axis = 1)\n\n#Renaming height column to height_cm for clarity\ndf = df.rename(columns = {\"height\": \"height_cm\"})\ndf\n\n\n\n\n\n\n\n\nyear\nplayer\ngp\ng\na\ntp\ndob\nheight_cm\nshoots\ndraft_year\nnhl_gp\nweight_kg\n\n\n\n\n0\n2004-2005\nBOBBY RYAN\n62\n37\n52\n89\n1987-03-17\n188\nR\n2005\n866\n95\n\n\n1\n2004-2005\nDAN RYDER\n68\n29\n53\n82\n1987-01-12\n180\nR\n2005\n0\n88\n\n\n2\n2004-2005\nCAL O'REILLY\n68\n23\n50\n73\n1986-09-30\n183\nL\n2005\n145\n85\n\n\n3\n2004-2005\nSTEVE DOWNIE\n61\n21\n52\n73\n1987-04-03\n180\nR\n2005\n434\n87\n\n\n4\n2004-2005\nEVAN BROPHEY\n63\n28\n43\n71\n1986-12-03\n185\nL\n2005\n4\n92\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n311\n2018-2019\nTYLER ANGLE\n58\n20\n24\n44\n2000-09-30\n178\nL\n2019\n4\n78\n\n\n312\n2018-2019\nNICHOLAS PORCO\n67\n20\n16\n36\n2001-03-12\n186\nL\n2019\n0\n85\n\n\n313\n2018-2019\nGRAEME CLARKE\n55\n23\n11\n34\n2001-04-24\n183\nR\n2019\n3\n79\n\n\n314\n2018-2019\nMASON PRIMEAU\n69\n13\n20\n33\n2001-07-28\n195\nL\n2019\n0\n92\n\n\n315\n2018-2019\nJAMIESON REES\n37\n10\n22\n32\n2001-02-26\n179\nL\n2019\n0\n84\n\n\n\n\n316 rows × 12 columns\n\n\n\nSTEP 3: TRANSFORM\nCreating some new stats (age at draft, goals per game, points per game, and an indicator variable for whether the player has played at least 200 NHL games). These will all be used in the regression later on.\n\n#Getting the age of the player at the time of draft (for simplicity, we will assume draft to be on June 30 for all years)\ndf[\"draft_date\"] = df[\"draft_year\"].astype(str) + '-06-30'\ndf[\"draft_date\"] = pd.to_datetime(df[\"draft_date\"])\ndf[\"dob\"] = pd.to_datetime(df[\"dob\"])\ndf[\"age_days\"] = (df[\"draft_date\"] - df[\"dob\"])\ndf[\"age_days\"] = df[\"age_days\"].dt.days\n\n#Can get rid of intermediate columns\ndf = df.drop([\"draft_date\", \"draft_year\", \"dob\"], axis = 1)\n\n#Adding columns for goals/g and points/g\ndf[\"gpg\"] = df[\"g\"] / df[\"gp\"]\ndf[\"apg\"] = df[\"a\"] / df[\"gp\"]\n\n#Create indicator variable to measure if the player has played at least 200 nhl games\ndf[\"Pr[GP &gt; 200]\"] = df[\"nhl_gp\"] &gt;= 200\ndf[\"Pr[GP &gt; 200]\"] = df[\"Pr[GP &gt; 200]\"].astype(int)\n\ndf\n\n\n\n\n\n\n\n\nyear\nplayer\ngp\ng\na\ntp\nheight_cm\nshoots\nnhl_gp\nweight_kg\nage_days\ngpg\napg\nPr[GP &gt; 200]\n\n\n\n\n0\n2004-2005\nBOBBY RYAN\n62\n37\n52\n89\n188\nR\n866\n95\n6680\n0.596774\n0.838710\n1\n\n\n1\n2004-2005\nDAN RYDER\n68\n29\n53\n82\n180\nR\n0\n88\n6744\n0.426471\n0.779412\n0\n\n\n2\n2004-2005\nCAL O'REILLY\n68\n23\n50\n73\n183\nL\n145\n85\n6848\n0.338235\n0.735294\n0\n\n\n3\n2004-2005\nSTEVE DOWNIE\n61\n21\n52\n73\n180\nR\n434\n87\n6663\n0.344262\n0.852459\n1\n\n\n4\n2004-2005\nEVAN BROPHEY\n63\n28\n43\n71\n185\nL\n4\n92\n6784\n0.444444\n0.682540\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n311\n2018-2019\nTYLER ANGLE\n58\n20\n24\n44\n178\nL\n4\n78\n6847\n0.344828\n0.413793\n0\n\n\n312\n2018-2019\nNICHOLAS PORCO\n67\n20\n16\n36\n186\nL\n0\n85\n6684\n0.298507\n0.238806\n0\n\n\n313\n2018-2019\nGRAEME CLARKE\n55\n23\n11\n34\n183\nR\n3\n79\n6641\n0.418182\n0.200000\n0\n\n\n314\n2018-2019\nMASON PRIMEAU\n69\n13\n20\n33\n195\nL\n0\n92\n6546\n0.188406\n0.289855\n0\n\n\n315\n2018-2019\nJAMIESON REES\n37\n10\n22\n32\n179\nL\n0\n84\n6698\n0.270270\n0.594595\n0\n\n\n\n\n316 rows × 14 columns\n\n\n\nSTEP 4: VISUALIZE\nWe will create 2 visualizations, to investiage whether certain variates are correlated with the probability of a player playing more than 200 games in the NHL.\n\nThe first visualization will be a scatterplot of goals per game in the OHL vs. NHL games played, colored by points per game.\nThe second visualization will be a scatterplot of height of players in the OHL vs. NHL games played, colored by weight in kg.\n\n\n(\n    ggplot(\n      data = df,\n      mapping = aes(\n        x = \"gpg\", y = \"nhl_gp\", color = \"apg\"\n      )\n    ) +\n      geom_point() +\n      labs(color = \"apg\")\n)\n\n   \n   \n\n\n\n(\n    ggplot(\n      data = df,\n      mapping = aes(\n        x = \"height_cm\", y = \"nhl_gp\", color = \"weight_kg\"\n      )\n    ) +\n      geom_point() +\n      labs(color = \"weight_kg\")\n)\n\n   \n   \n\n\nWe can see a little bit or correlation in both graphs. In the nwxt step, we will use regression to try and come with a better model that better quanitifies the impact of multiple variables on the probability of a player playing more than 200 games in the NHL.\nSTEP 5: MODEL\nWe are going to create a logistic regression model. Logistic regression is used since we are regressing for a probability, which needs to be bounded between 0 and 1. The model will aim to predict the probability a player becoming an everyday NHL player (play 200 NHL games or more):\n\n# X: predictors, y: binary response\ndf_regress = df[df[\"year\"].isin([\"2004-2005\", \"2006-2007\", \"2007-2008\", \"2008-2009\", \"2009-2010\", \n                                 \"2014-2015\", \"2015-2016\", \"2016-2017\", \"2017-2018\", \"2018-2019\"])]\nX = df_regress[['gp', 'height_cm', 'weight_kg', 'age_days', 'gpg', 'apg']]\nX = sm.add_constant(X)  # adds intercept\ny = df_regress['Pr[GP &gt; 200]']\n\nmodel = sm.Logit(y, X)\nresult = model.fit()\n\n#Print the coefficients outputted by model\nprint(result.params)\n\nOptimization terminated successfully.\n         Current function value: 0.508646\n         Iterations 6\nconst        2.711834\ngp          -0.008993\nheight_cm    0.052710\nweight_kg    0.013123\nage_days    -0.002453\ngpg          1.584886\napg          3.683353\ndtype: float64\n\n\nSTEP 6: COMMUNICATE\nLet’s first look at if there is multicollinearity in our data:\n\nprint(X.corr())\n\n           const        gp  height_cm  weight_kg  age_days       gpg       apg\nconst        NaN       NaN        NaN        NaN       NaN       NaN       NaN\ngp           NaN  1.000000  -0.002658   0.016605  0.078796  0.039888  0.006051\nheight_cm    NaN -0.002658   1.000000   0.632546 -0.114276 -0.227014 -0.281900\nweight_kg    NaN  0.016605   0.632546   1.000000  0.001808 -0.097414 -0.148366\nage_days     NaN  0.078796  -0.114276   0.001808  1.000000  0.179263  0.168196\ngpg          NaN  0.039888  -0.227014  -0.097414  0.179263  1.000000  0.680676\napg          NaN  0.006051  -0.281900  -0.148366  0.168196  0.680676  1.000000\n\n\nMulticollinearity would be suggested by a value close to 1 or -1. Aside form the diagonal values, which are always 1, we can see that there is no multicollinearity in our data.\nNow, we will test out our model on a test set. We will use the model to predict the probability of a player becoming an everyday NHL player (playing 200 or more NHL games). We will do so on seasons we did not train our model on.\n\n#Testing model out\ndf_test = df[df[\"year\"].isin([\"2010-2011\", \"2011-2012\", \"2012-2013\", \"2013-2014\"])]\nX = df_test[['gp', 'height_cm', 'weight_kg', 'age_days', 'gpg', 'apg']]\nX = sm.add_constant(X, has_constant='add')  # adds intercept\n\n#If regression yields a probability greater than or equal to 0.5, we will say it predicts the player will become a full-time NHL player\ndf_test[\"pred_prob\"] = result.predict(X) &gt;= 0.5\ndf_test\n\n\n\n\n\n\n\n\nyear\nplayer\ngp\ng\na\ntp\nheight_cm\nshoots\nnhl_gp\nweight_kg\nage_days\ngpg\napg\nPr[GP &gt; 200]\npred_prob\n\n\n\n\n120\n2010-2011\nRYAN STROME\n65\n33\n73\n106\n185\nR\n864\n87\n6563\n0.507692\n1.123077\n1\nTrue\n\n\n121\n2010-2011\nSHANE PRINCE\n59\n25\n63\n88\n181\nL\n128\n88\n6800\n0.423729\n1.067797\n0\nTrue\n\n\n122\n2010-2011\nSTEFAN NOESEN\n68\n34\n43\n77\n185\nR\n444\n93\n6712\n0.500000\n0.632353\n1\nFalse\n\n\n123\n2010-2011\nANDY ANDREOFF\n66\n33\n42\n75\n185\nL\n188\n95\n7349\n0.500000\n0.636364\n0\nFalse\n\n\n124\n2010-2011\nMARK SCHEIFELE\n66\n22\n53\n75\n190\nR\n879\n94\n6681\n0.333333\n0.803030\n1\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n216\n2013-2014\nCRISTIANO DIGIACINTO\n50\n17\n11\n28\n183\nL\n0\n88\n6746\n0.340000\n0.220000\n0\nFalse\n\n\n217\n2013-2014\nJAKE EVANS\n57\n11\n7\n18\n183\nL\n350\n89\n7072\n0.192982\n0.122807\n1\nFalse\n\n\n218\n2013-2014\nJADEN LINDO\n40\n9\n9\n18\n188\nR\n0\n97\n6745\n0.225000\n0.225000\n0\nFalse\n\n\n219\n2013-2014\nCHRISTIAN DVORAK\n33\n6\n8\n14\n185\nL\n534\n91\n6723\n0.181818\n0.242424\n1\nFalse\n\n\n220\n2013-2014\nKYLE PETTIT\n53\n5\n5\n10\n193\nL\n0\n91\n6737\n0.094340\n0.094340\n0\nFalse\n\n\n\n\n101 rows × 15 columns\n\n\n\nNext, we will look at model accuracy. We will use TP, TN, FP, FN to calculate accuracy.\nFinally, we will look at the ROC curve and AUC to evaluate the model’s performance.\n\nTP = df_test[(df_test[\"Pr[GP &gt; 200]\"] == True) & (df_test[\"pred_prob\"] == True)].shape[0]\nprint(f\"True Positive: {TP}\")\n\nTN = df_test[(df_test[\"Pr[GP &gt; 200]\"] == False) & (df_test[\"pred_prob\"] == False)].shape[0]\nprint(f\"True Negative: {TN}\")\n\nFP = df_test[(df_test[\"Pr[GP &gt; 200]\"] == False) & (df_test[\"pred_prob\"] == True)].shape[0]\nprint(f\"False Positive: {FP}\")\n\nFN = df_test[(df_test[\"Pr[GP &gt; 200]\"] == True) & (df_test[\"pred_prob\"] == False)].shape[0]\nprint(f\"False Negative: {FN}\")\n\naccuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) &gt; 0 else 0\nprint(f\"Accuracy: {accuracy:.2%}\")\n\nTrue Positive: 7\nTrue Negative: 61\nFalse Positive: 2\nFalse Negative: 31\nAccuracy: 67.33%\n\n\nAn accuracy of 67.33% is decent. What would happen if we changed the threshold for a positive to a value that isn;t 0.5? We can test this using a ROC curve. The ROC curve shows the trade-off between true positive rate and false positive rate at different thresholds. The AUC (Area Under the Curve) is a single number that summarizes the performance of the model across all thresholds.\nWe will also use ROC curve to look at the TPR, FPR of different thresholds. We can calculate AUR from this to measure accuracy of model considering all thresholds (AUS = 1 means perfect model, AUS = 0.5 means random guessing).:\n\n# True labels and predicted probabilities\ny_true = df_test[\"Pr[GP &gt; 200]\"]\ny_score = result.predict(X)\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_true, y_score)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\nAn AUC of 0.7 is significantly better than random guessing. We see that our model does a pretty good job at predicting full-time NHL career probability based on OHL performance and player build.\nSTORING MODEL IN AWS S3 BUCKET\nThe code below will store the model in an AWS S3 bucket. I had to make a custom handler, since vetiver cannot handle sm.Logit models by default.\n\n#Need to create a custom handler for statsmodels Logit models, as the default handler cannot handle this type of model\nclass StatsmodelsLogitHandler(BaseHandler):\n    def __init__(self, model, prototype_data):\n        super().__init__(model, prototype_data)\n\n    @staticmethod\n    def model_type():\n        return \"statsmodels_logit\"\n\n    pip_name = \"statsmodels\"\n\n    def handler_predict(self, input_data):\n        # Add constant to match model spec\n        input_data_const = sm.add_constant(input_data, has_constant='add')\n        prediction = self.model.predict(input_data_const)\n        return prediction\n\n\n#Store the model in an S3 bucket:\nboard = board_s3(\"devakshah-stat468-models\", allow_pickle_read=True)\n\ncustom_handler = StatsmodelsLogitHandler(result, prototype_data=X)\nvetiver_model = VetiverModel(custom_handler, model_name=\"my_logit_model\", description=\"Logistic regression\", handler=custom_handler)\n\nvetiver_pin_write(board, vetiver_model)\n\nModel Cards provide a framework for transparent, responsible reporting. \n Use the vetiver `.qmd` Quarto template as a place to start, \n with vetiver.model_card()\n('The hash of pin \"my_logit_model\" has not changed. Your pin will not be stored.',)\n\n\nThe code below is used to store other data in the AWS S3 bucket. This data will be imported into the Shiny App.\n\n#Storing Other Data used by app / report in s3 bucket:\nboard = board_s3(\"devakshah-stat468-models\", allow_pickle_read=True)\nboard.pin_write(df, name=\"regression_input_data\", type=\"csv\")\n\n('The hash of pin \"regression_input_data\" has not changed. Your pin will not be stored.',)\n\n\nMeta(title='regression_input_data: a pinned 316 x 14 DataFrame', description=None, created='20250810T214442Z', pin_hash='5c9e6f009ed63f9c', file='regression_input_data.csv', file_size=28477, type='csv', api_version=1, version=Version(created=datetime.datetime(2025, 8, 10, 21, 44, 42), hash='5c9e6'), tags=None, name='regression_input_data', user={}, local={})\n\n\n\npins = board.pin_list()\nprint(pins)\n\n['my_logit_model', 'prospects_2020_to_2025_data', 'regression_input_data']"
  },
  {
    "objectID": "old_final_project.html",
    "href": "old_final_project.html",
    "title": "STAT468 Final Project Report",
    "section": "",
    "text": "import TopDownHockey_Scraper.TopDownHockey_NHL_Scraper as tdhnhlscrape\nimport TopDownHockey_Scraper.TopDownHockey_EliteProspects_Scraper as tdhepscrape\nfrom nhlpy import NHLClient\n\nWelcome to the TopDownHockey NHL Scraper, built by Patrick Bacon.\nIf you enjoy the scraper and would like to support my work, or you have any comments, questions, or concerns, feel free to follow me on Twitter @TopDownHockey or reach out to me via email at patrick.s.bacon@gmail.com. Have fun!\nWelcome to the TopDownHockey EliteProspects Scraper, built by Patrick Bacon.\nThis scraper is built strictly for personal use. For commercial or professional use, please look into the EliteProspects API.\nIf you enjoy the scraper and would like to support my work, feel free to follow me on Twitter @TopDownHockey. Have fun!\n\n\n\nimport pandas as pd\nfrom datetime import date\nimport statsmodels.api as sm\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\n\n#Toggle to see if we want to rescrape, or just import prior scrape from existing file\nscrape = False\ndevak = True\nfilename = \"regression_input.xlsx\"\n\n\n#Get OHL Player Data (stats, build, etc)\n\nif scrape == True:\n    #Can't include all years because of backend data issues in some years\n    #years = [\"2004-2005\", \"2006-2007\", \"2007-2008\", \"2008-2009\", \"2009-2010\", \"2010-2011\", \"2011-2012\", \"2012-2013\", \"2013-2014\", \n    #         \"2014-2015\", \"2015-2016\", \"2016-2017\", \"2017-2018\", \"2018-2019\"]\n\n    years = [\"2016-2017\", \"2013-2014\"]\n\n    aggregated_output = pd.DataFrame()\n\n    for year in years:\n        df = tdhepscrape.get_skaters((\"ohl\"), (year))[0:10]\n\n        #GET PLAYER INFO\n        info = tdhepscrape.get_player_information(df)\n\n        #GET RID OF D-MEN\n        df = df[~df['player'].str.contains(r'\\(([^)]*D[^)]*)\\)', regex=True)]\n\n        #GET RID OF PLAYER POSITIONS FROM NAMES\n        df['player'] = df['player'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n\n        #ADD YEAR TO DF\n        df.insert(0, \"year\", year)\n\n        #JOIN PLAYER BIO WITH STATS\n        year_output = pd.merge(df[[\"year\", \"player\", \"gp\", \"g\", \"a\", \"tp\"]], info[[\"player\", \"dob\", \"height\", \"weight\", \"shoots\"]], on='player', how='inner')\n\n        #ADD CURRENT YEAR PROSPECTS TO AGGREGATED DF\n        aggregated_output = pd.concat([aggregated_output, year_output])\n\n\n#Changing types of all columns, they are objects by default\nif scrape == True:\n    aggregated_output[\"gp\"] = aggregated_output[\"gp\"].replace(\"-\", 0)\n    aggregated_output[\"gp\"] = aggregated_output[\"gp\"].astype(int)\n    aggregated_output[\"g\"] = aggregated_output[\"g\"].replace(\"-\", 0)\n    aggregated_output[\"g\"] = aggregated_output[\"g\"].astype(int)\n    aggregated_output[\"a\"] = aggregated_output[\"a\"].replace(\"-\", 0)\n    aggregated_output[\"a\"] = aggregated_output[\"a\"].astype(int)\n    aggregated_output[\"tp\"] = aggregated_output[\"tp\"].replace(\"-\", 0)\n    aggregated_output[\"tp\"] = aggregated_output[\"tp\"].astype(int)\n\n\n#Get corresponding draft year for every season (oe. 2024-2025 would have a draft year of 2025)\n# note this isn't necessarily the players' draft year - it's just the draft year for the corresponding row's season\n\nif scrape == True:\n    aggregated_output[\"draft_year\"] = aggregated_output[\"year\"].str[5:]\n    aggregated_output[\"draft_year\"] = aggregated_output[\"draft_year\"].astype(int)\n    aggregated_output\n\n\n#Getting all NHL players drafted form 2005 - 2020\nif scrape == True:\n    years = list(range(2005, 2021))\n\n    draftyears = pd.DataFrame()\n\n    for year in years:\n        df_list = pd.read_html(f\"https://www.hockey-reference.com/draft/NHL_{year}_entry.html\", match=\"Round\")\n\n        players_drafted = df_list[0]\n\n        players_drafted\n\n        #Let's get rid of the top header that isnt really used\n        players_drafted.columns = players_drafted.columns.get_level_values(1)\n\n        players_drafted[\"draft_year\"] = year\n        players_drafted = players_drafted[[\"draft_year\", \"Player\"]]\n        players_drafted =players_drafted.rename(columns={\"draft_year\": \"player_draft_year\", \"Player\": \"player\"})\n        draftyears = pd.concat([draftyears, players_drafted])\n\n    draftyears        \n\n\n#Getting games played for all NHL players\nif scrape == True:\n    nhl_gp = pd.DataFrame()\n    pages = list(range(1, 80))\n\n    for page in pages:\n        df_list = pd.read_html(f\"https://www.eliteprospects.com/league/nhl/stats/all-time?page={page}\")\n        page_stats = df_list[2]\n        page_stats = page_stats[[\"Player\", \"GP\"]]\n        nhl_gp = pd.concat([nhl_gp, page_stats])\n\n    nhl_gp = nhl_gp.rename(columns={\"Player\": \"player\", \"GP\": \"nhl_gp\"})\n\n    #GET RID OF PLAYER POSITIONS FROM NAMES\n    nhl_gp['player'] = nhl_gp['player'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n\n    # Replacing NA and \"-\" values with 0\n    nhl_gp[\"nhl_gp\"] = nhl_gp[\"nhl_gp\"].fillna(0)\n    nhl_gp[\"nhl_gp\"] = nhl_gp[\"nhl_gp\"].replace(\"-\", 0)\n\n    nhl_gp\n\n\n#Join GP, draft year onto OHL player data\nif scrape == True:\n    #MAKE ALL PLAYER NAMES UPPERCASE (TO MAKE JOINING TABLES NON CASE SENSITIVE)\n    aggregated_output['player'] = aggregated_output['player'].str.upper()\n    draftyears['player'] = draftyears['player'].str.upper()\n    nhl_gp['player'] = nhl_gp['player'].str.upper()\n\n\n    #Filter ohl stats for only drafted players' draft year stats - \n    #This will get rid of a) undrafted players, and b) drafted players non-draft year stats\n    df = pd.merge(aggregated_output, draftyears, left_on=['player', 'draft_year'], right_on=['player', 'player_draft_year'], how='inner')\n\n    #Can get rid of one of the draft year columns - don't need both\n    df = df[[\"year\", \"player\", \"gp\", \"g\", \"a\", \"tp\", \"dob\", \"height\", \"weight\", \"shoots\", 'draft_year']]\n\n    #Join players' games played - if player gp not found, assume it to be 0.\n    df = pd.merge(df, nhl_gp, left_on=['player'], right_on=['player'], how='left')\n    df[\"nhl_gp\"] = df[\"nhl_gp\"].fillna(0)\n    df[\"nhl_gp\"] = df[\"nhl_gp\"].astype(int)\n\n    df.to_excel(filename, index=False)\n\n\n#SOME DATA CLEANING\n\ndf = pd.read_excel(filename)\n\n#Changing the weight to a numerical variate in kg\ndf[\"weight_kg\"] = df[\"weight\"].apply(lambda x: x[:x.find(\" \")])\ndf[\"weight_kg\"] = df[\"weight_kg\"].astype(int)\ndf.dtypes\n\n#Getting the age of the player at the time of draft (for simplicity, we will assume draft to be on June 30 for all years)\n\ndf[\"draft_date\"] = df[\"draft_year\"].astype(str) + '-06-30'\ndf[\"draft_date\"] = pd.to_datetime(df[\"draft_date\"])\ndf[\"dob\"] = pd.to_datetime(df[\"dob\"])\ndf[\"age_days\"] = (df[\"draft_date\"] - df[\"dob\"])\ndf[\"age_days\"] = df[\"age_days\"].dt.days\n\n#Can get rid of intermediate columns\ndf = df.drop([\"dob\", \"weight\", \"draft_year\", \"draft_date\"], axis = 1)\n\n#Adding columns for goals/g and points/g\ndf[\"gpg\"] = df[\"g\"] / df[\"gp\"]\ndf[\"ppg\"] = df[\"tp\"] / df[\"gp\"]\n\n#Create indicator variable to measure if the player has played at least 200 nhl games\ndf[\"Pr[GP &gt; 200]\"] = df[\"nhl_gp\"] &gt;= 200\ndf[\"Pr[GP &gt; 200]\"].astype(int)\n\n#Renaming height column to height_cm for clarity\ndf = df.rename(columns = {\"height\": \"height_cm\"})\ndf\n\n\n\n\n\n\n\n\nyear\nplayer\ngp\ng\na\ntp\nheight_cm\nshoots\nnhl_gp\nweight_kg\nage_days\ngpg\nppg\nPr[GP &gt; 200]\n\n\n\n\n0\n2006-2007\nPATRICK KANE\n58\n62\n83\n145\n178\nL\n1302\n80\n6797\n1.068966\n2.500000\nTrue\n\n\n1\n2006-2007\nSAM GAGNER\n53\n35\n83\n118\n180\nR\n1043\n89\n6533\n0.660377\n2.226415\nTrue\n\n\n2\n2006-2007\nBRETT MACLEAN\n68\n47\n53\n100\n188\nR\n18\n89\n6762\n0.691176\n1.470588\nFalse\n\n\n3\n2006-2007\nSTEFAN LEGEIN\n64\n43\n32\n75\n178\nR\n0\n77\n6792\n0.671875\n1.171875\nFalse\n\n\n4\n2006-2007\nZACK TORQUATO\n65\n30\n39\n69\n183\nR\n0\n88\n6596\n0.461538\n1.061538\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n283\n2019-2020\nDECLAN MCDONNELL\n63\n21\n21\n42\n178\nR\n0\n86\n6700\n0.333333\n0.666667\nFalse\n\n\n284\n2019-2020\nANTONIO STRANGES\n61\n19\n21\n40\n180\nL\n0\n84\n6720\n0.311475\n0.655738\nFalse\n\n\n285\n2019-2020\nTANNER DICKINSON\n64\n9\n31\n40\n183\nL\n0\n80\n6692\n0.140625\n0.625000\nFalse\n\n\n286\n2019-2020\nMARTIN CHROMIAK\n28\n11\n22\n33\n183\nR\n0\n86\n6524\n0.392857\n1.178571\nFalse\n\n\n287\n2019-2020\nJAN MYSAK\n22\n15\n10\n25\n180\nL\n0\n86\n6581\n0.681818\n1.136364\nFalse\n\n\n\n\n288 rows × 14 columns\n\n\n\n\n# X: predictors, y: binary response\ndf_regress = df[df[\"year\"].isin([\"2004-2005\", \"2006-2007\", \"2007-2008\", \"2008-2009\", \"2009-2010\", \n                                 \"2014-2015\", \"2015-2016\", \"2016-2017\", \"2017-2018\", \"2018-2019\"])]\nX = df_regress[['g', 'a', 'tp', 'height_cm', 'weight_kg', 'age_days', 'gpg', 'ppg']]\nX = sm.add_constant(X)  # adds intercept\ny = df_regress['Pr[GP &gt; 200]']\n\nmodel = sm.Logit(y, X)\nresult = model.fit()\n\nprint(result.summary())\n\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.488150\n         Iterations: 35\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:           Pr[GP &gt; 200]   No. Observations:                  167\nModel:                          Logit   Df Residuals:                      158\nMethod:                           MLE   Df Model:                            8\nDate:                Mon, 04 Aug 2025   Pseudo R-squ.:                  0.2244\nTime:                        20:14:31   Log-Likelihood:                -81.521\nconverged:                      False   LL-Null:                       -105.11\nCovariance Type:            nonrobust   LLR p-value:                 1.424e-07\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.7392     10.224     -0.072      0.942     -20.778      19.299\ng             -0.0991        nan        nan        nan         nan         nan\na              0.0749        nan        nan        nan         nan         nan\ntp            -0.0238        nan        nan        nan         nan         nan\nheight_cm      0.0524      0.047      1.109      0.267      -0.040       0.145\nweight_kg      0.0318      0.040      0.801      0.423      -0.046       0.110\nage_days      -0.0023      0.001     -2.140      0.032      -0.004      -0.000\ngpg            7.3258     22.332      0.328      0.743     -36.444      51.095\nppg            1.4341      9.650      0.149      0.882     -17.480      20.349\n==============================================================================\n\n\n/Users/devakshah/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\n\n#Testing model out\ndf_test = df[df[\"year\"].isin([\"2010-2011\", \"2011-2012\", \"2012-2013\", \"2013-2014\"])]\nX = df_test[['g', 'a', 'tp', 'height_cm', 'weight_kg', 'age_days', 'gpg', 'ppg']]\nX = sm.add_constant(X, has_constant='add')  # adds intercept\n\n#If regression yields a probability greater than or equal to 0.5, we will say it predicts the player will become a full-time NHL player\ndf_test[\"pred_prob\"] = result.predict(X) &gt;= 0.5\ndf_test\n\n\n\n\n\n\n\n\nyear\nplayer\ngp\ng\na\ntp\nheight_cm\nshoots\nnhl_gp\nweight_kg\nage_days\ngpg\nppg\nPr[GP &gt; 200]\npred_prob\n\n\n\n\n72\n2010-2011\nRYAN STROME\n65\n33\n73\n106\n185\nR\n864\n87\n6563\n0.507692\n1.630769\nTrue\nTrue\n\n\n73\n2010-2011\nSHANE PRINCE\n59\n25\n63\n88\n181\nL\n128\n88\n6800\n0.423729\n1.491525\nFalse\nTrue\n\n\n74\n2010-2011\nSTEFAN NOESEN\n68\n34\n43\n77\n185\nR\n444\n93\n6712\n0.500000\n1.132353\nTrue\nFalse\n\n\n75\n2010-2011\nANDY ANDREOFF\n66\n33\n42\n75\n185\nL\n188\n95\n7349\n0.500000\n1.136364\nFalse\nFalse\n\n\n76\n2010-2011\nMARK SCHEIFELE\n66\n22\n53\n75\n190\nR\n879\n94\n6681\n0.333333\n1.136364\nTrue\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n168\n2013-2014\nCRISTIANO DIGIACINTO\n50\n17\n11\n28\n183\nL\n0\n88\n6746\n0.340000\n0.560000\nFalse\nFalse\n\n\n169\n2013-2014\nJAKE EVANS\n57\n11\n7\n18\n183\nL\n350\n89\n7072\n0.192982\n0.315789\nTrue\nFalse\n\n\n170\n2013-2014\nJADEN LINDO\n40\n9\n9\n18\n188\nR\n0\n97\n6745\n0.225000\n0.450000\nFalse\nFalse\n\n\n171\n2013-2014\nCHRISTIAN DVORAK\n33\n6\n8\n14\n185\nL\n534\n91\n6723\n0.181818\n0.424242\nTrue\nFalse\n\n\n172\n2013-2014\nKYLE PETTIT\n53\n5\n5\n10\n193\nL\n0\n91\n6737\n0.094340\n0.188679\nFalse\nFalse\n\n\n\n\n101 rows × 15 columns\n\n\n\n\nTP = df_test[(df_test[\"Pr[GP &gt; 200]\"] == True) & (df_test[\"pred_prob\"] == True)].shape[0]\nprint(f\"True Positive: {TP}\")\n\nTN = df_test[(df_test[\"Pr[GP &gt; 200]\"] == False) & (df_test[\"pred_prob\"] == False)].shape[0]\nprint(f\"True Negative: {TN}\")\n\nFP = df_test[(df_test[\"Pr[GP &gt; 200]\"] == False) & (df_test[\"pred_prob\"] == True)].shape[0]\nprint(f\"False Positive: {FP}\")\n\nFN = df_test[(df_test[\"Pr[GP &gt; 200]\"] == True) & (df_test[\"pred_prob\"] == False)].shape[0]\nprint(f\"False Negative: {FN}\")\n\nsuccess_rate = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) &gt; 0 else 0\nprint(f\"Success Rate: {success_rate:.2%}\")\n\npositive_success_rate = TP / (TP + FP) if (TP + FP) &gt; 0 else 0\nprint(f\"Positive Success Rate: {positive_success_rate:.2%}\")\n\nnhler_success_rate = TP / (TP + FN) if (TP + FN) &gt; 0 else 0\nprint(f\"NHLer Success Rate: {nhler_success_rate:.2%}\")\n\nTrue Positive: 10\nTrue Negative: 59\nFalse Positive: 4\nFalse Negative: 28\nSuccess Rate: 68.32%\nPositive Success Rate: 71.43%\nNHLer Success Rate: 26.32%\n\n\n\nfrom vetiver.handlers.base import BaseHandler\nimport statsmodels.api as sm\n\n#Need to create a custom handler for statsmodels Logit models, as the default handler cannot handle this type of model\nclass StatsmodelsLogitHandler(BaseHandler):\n    def __init__(self, model, prototype_data):\n        super().__init__(model, prototype_data)\n\n    @staticmethod\n    def model_type():\n        return \"statsmodels_logit\"\n\n    pip_name = \"statsmodels\"\n\n    def handler_predict(self, input_data, check_prototype: bool):\n        \"\"\"\n        Make predictions using a fitted statsmodels Logit model.\n\n        Parameters\n        ----------\n        input_data:\n            New data (e.g., from API)\n        check_prototype: bool\n            Whether to check data shape\n\n        Returns\n        -------\n        Prediction array from model.predict\n        \"\"\"\n        # Add constant to match model spec\n        input_data_const = sm.add_constant(input_data, has_constant='add')\n        prediction = self.model.predict(input_data_const)\n        return prediction\n\n\nfrom pins import board_s3\nfrom vetiver import vetiver_pin_write\nfrom vetiver import VetiverModel\n\n#Store the model in an S3 bucket:\n\nboard = board_s3(\"devakshah-stat468-models\", allow_pickle_read=True)\n\ncustom_handler = StatsmodelsLogitHandler(result, prototype_data=X)\nvetiver_model = VetiverModel(custom_handler, model_name=\"my_logit_model\", description=\"Logistic regression\", handler=custom_handler)\n\nvetiver_pin_write(board, vetiver_model)\n\nModel Cards provide a framework for transparent, responsible reporting. \n Use the vetiver `.qmd` Quarto template as a place to start, \n with vetiver.model_card()\n('The hash of pin \"my_logit_model\" has not changed. Your pin will not be stored.',)\n\n\n\n#Storing Other Data used by app / report in s3 bucket:\nboard = board_s3(\"devakshah-stat468-models\", allow_pickle_read=True)\nboard.pin_write(df, name=\"regression_input_data\", type=\"csv\")\n\nWriting pin:\nName: 'regression_input_data'\nVersion: 20250804T201549Z-c3a37\n\n\nMeta(title='regression_input_data: a pinned 288 x 14 DataFrame', description=None, created='20250804T201549Z', pin_hash='c3a374df234b2935', file='regression_input_data.csv', file_size=26991, type='csv', api_version=1, version=Version(created=datetime.datetime(2025, 8, 4, 20, 15, 49, 442041), hash='c3a374df234b2935'), tags=None, name='regression_input_data', user={}, local={})"
  },
  {
    "objectID": "prospects_2020_to_2025.html",
    "href": "prospects_2020_to_2025.html",
    "title": "",
    "section": "",
    "text": "import TopDownHockey_Scraper.TopDownHockey_NHL_Scraper as tdhnhlscrape\nimport TopDownHockey_Scraper.TopDownHockey_EliteProspects_Scraper as tdhepscrape\nfrom nhlpy import NHLClient\nimport pandas as pd\nfrom datetime import date\nimport statsmodels.api as sm\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom lets_plot import *\nLetsPlot.setup_html()\nfrom pins import board_s3\n\n\n            \n            \n            \n\n\n\nscrape = False # Set to True to scrape data, False to use existing data\n\n#This will be the file name where data is stored in our git repo\nfilename = \"prospects_2020_to_2025.xlsx\"\n\n\n#Get OHL Player Data (stats, build, etc)\n\nif scrape == True:\n\n    #Can't include all years because of backend API issues in some years\n    years = [\"2019-2020\", \"2020-2021\", \"2021-2022\", \"2022-2023\", \"2023-2024\", \"2024-2025\"]\n\n    aggregated_output = pd.DataFrame()\n\n    for year in years:\n        df = tdhepscrape.get_skaters((\"ohl\"), (year))\n\n        #GET PLAYER INFO\n        info = tdhepscrape.get_player_information(df)\n        \n        #GET RID OF DEFENCEMEN\n        df = df[~df['player'].str.contains(r'\\(([^)]*D[^)]*)\\)', regex=True)]\n\n        #GET RID OF PLAYER POSITIONS FROM NAMES\n        df['player'] = df['player'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n\n        #ADD YEAR TO DF\n        df.insert(0, \"year\", year)\n\n        #JOIN PLAYER BIO WITH STATS\n        year_output = pd.merge(df[[\"year\", \"player\", \"gp\", \"g\", \"a\", \"tp\"]], info[[\"player\", \"dob\", \"height\", \"weight\", \"shoots\"]], on='player', how='inner')\n\n        #ADD CURRENT YEAR PROSPECTS TO AGGREGATED DF\n        aggregated_output = pd.concat([aggregated_output, year_output])\n\n\nif scrape == True:\n    years = list(range(2020, 2026))\n\n    draftyears = pd.DataFrame()\n\n    for year in years:\n        df_list = pd.read_html(f\"https://www.hockey-reference.com/draft/NHL_{year}_entry.html\", match=\"Round\")\n\n        players_drafted = df_list[0]\n\n        players_drafted\n\n        #Let's get rid of the top header that isnt really used\n        players_drafted.columns = players_drafted.columns.get_level_values(1)\n\n        players_drafted[\"draft_year\"] = year\n        players_drafted = players_drafted[[\"draft_year\", \"Player\"]]\n        players_drafted =players_drafted.rename(columns={\"draft_year\": \"player_draft_year\", \"Player\": \"player\"})\n        draftyears = pd.concat([draftyears, players_drafted])\n\n    print(draftyears)    \n\n\n#Getting games played for all NHL players\nif scrape == True:\n    nhl_gp = pd.DataFrame()\n    pages = list(range(1, 80))\n\n    for page in pages:\n        df_list = pd.read_html(f\"https://www.eliteprospects.com/league/nhl/stats/all-time?page={page}\")\n        page_stats = df_list[2]\n        page_stats = page_stats[[\"Player\", \"GP\"]]\n        nhl_gp = pd.concat([nhl_gp, page_stats])\n\n    nhl_gp = nhl_gp.rename(columns={\"Player\": \"player\", \"GP\": \"nhl_gp\"})\n\n    #GET RID OF PLAYER POSITIONS FROM NAMES\n    nhl_gp['player'] = nhl_gp['player'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n\n    # Replacing NA and \"-\" values with 0\n    nhl_gp[\"nhl_gp\"] = nhl_gp[\"nhl_gp\"].fillna(0)\n    nhl_gp[\"nhl_gp\"] = nhl_gp[\"nhl_gp\"].replace(\"-\", 0)\n\n    nhl_gp\n\n\nif scrape == True:\n    aggregated_output[\"gp\"] = aggregated_output[\"gp\"].replace(\"-\", 0)\n    aggregated_output[\"gp\"] = aggregated_output[\"gp\"].astype(int)\n    aggregated_output[\"g\"] = aggregated_output[\"g\"].replace(\"-\", 0)\n    aggregated_output[\"g\"] = aggregated_output[\"g\"].astype(int)\n    aggregated_output[\"a\"] = aggregated_output[\"a\"].replace(\"-\", 0)\n    aggregated_output[\"a\"] = aggregated_output[\"a\"].astype(int)\n    aggregated_output[\"tp\"] = aggregated_output[\"tp\"].replace(\"-\", 0)\n    aggregated_output[\"tp\"] = aggregated_output[\"tp\"].astype(int)\n\n\nif scrape == True:\n    aggregated_output[\"draft_year\"] = aggregated_output[\"year\"].str[5:]\n    aggregated_output[\"draft_year\"] = aggregated_output[\"draft_year\"].astype(int)\n    aggregated_output\n\n\n#Join GP, draft year onto OHL player data\nif scrape == True:\n    #MAKE ALL PLAYER NAMES UPPERCASE (TO MAKE JOINING TABLES NON CASE SENSITIVE)\n    aggregated_output['player'] = aggregated_output['player'].str.upper()\n    draftyears['player'] = draftyears['player'].str.upper()\n    # nhl_gp['player'] = nhl_gp['player'].str.upper()\n\n\n    #Filter ohl stats for only drafted players' draft year stats - \n    #This will get rid of a) undrafted players, and b) drafted players non-draft year stats\n    df = pd.merge(aggregated_output, draftyears, left_on=['player', 'draft_year'], right_on=['player', 'player_draft_year'], how='inner')\n\n    #Can get rid of one of the draft year columns - don't need both\n    df = df[[\"year\", \"player\", \"gp\", \"g\", \"a\", \"tp\", \"dob\", \"height\", \"weight\", \"shoots\", 'draft_year']]\n\n    #Join players' games played - if player gp not found, assume it to be 0.\n    df = pd.merge(df, nhl_gp, left_on=['player'], right_on=['player'], how='left')\n    df[\"nhl_gp\"] = df[\"nhl_gp\"].fillna(0)\n    df[\"nhl_gp\"] = df[\"nhl_gp\"].astype(int)\n\n    df.to_excel(filename, index=False)\n\n\ndf = pd.read_excel(filename)\n\n#Changing the weight to a numerical variate in kg\ndf[\"weight_kg\"] = df[\"weight\"].apply(lambda x: x[:x.find(\" \")])\ndf[\"weight_kg\"] = df[\"weight_kg\"].astype(int)\ndf.dtypes\n\n#Can get rid of intermediate columns\ndf = df.drop([\"weight\"], axis = 1)\n\n#Renaming height column to height_cm for clarity\ndf = df.rename(columns = {\"height\": \"height_cm\"})\ndf\n\n\n\n\n\n\n\n\nyear\nplayer\ngp\ng\na\ntp\ndob\nheight_cm\nshoots\ndraft_year\nnhl_gp\nweight_kg\n\n\n\n\n0\n2019-2020\nMARCO ROSSI\n56\n39\n81\n120\n2001-09-23\n176\nL\n2020\n0\n87\n\n\n1\n2019-2020\nCOLE PERFETTI\n61\n37\n74\n111\n2002-01-01\n180\nL\n2020\n0\n84\n\n\n2\n2019-2020\nJACK QUINN\n62\n52\n37\n89\n2001-09-19\n185\nR\n2020\n0\n84\n\n\n3\n2019-2020\nQUINTON BYFIELD\n45\n32\n50\n82\n2002-08-19\n195\nL\n2020\n0\n102\n\n\n4\n2019-2020\nTYSON FOERSTER\n62\n36\n44\n80\n2002-01-18\n188\nR\n2020\n0\n97\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n111\n2024-2025\nHARRY NANSI\n67\n7\n16\n23\n2007-09-10\n190\nR\n2025\n0\n84\n\n\n112\n2024-2025\nJORDAN CHARRON\n48\n10\n11\n21\n2007-06-21\n185\nL\n2025\n0\n86\n\n\n113\n2024-2025\nMATTHEW HLACAR\n43\n7\n1\n8\n2006-05-31\n191\nL\n2025\n0\n91\n\n\n114\n2024-2025\nKIEREN DERVIN\n10\n1\n2\n3\n2007-03-31\n186\nL\n2025\n0\n83\n\n\n115\n2024-2025\nPARKER HOLMES\n21\n1\n0\n1\n2007-03-08\n193\nL\n2025\n0\n101\n\n\n\n\n116 rows × 12 columns\n\n\n\n\n#Getting the age of the player at the time of draft (for simplicity, we will assume draft to be on June 30 for all years)\ndf[\"draft_date\"] = df[\"draft_year\"].astype(str) + '-06-30'\ndf[\"draft_date\"] = pd.to_datetime(df[\"draft_date\"])\ndf[\"dob\"] = pd.to_datetime(df[\"dob\"])\ndf[\"age_days\"] = (df[\"draft_date\"] - df[\"dob\"])\ndf[\"age_days\"] = df[\"age_days\"].dt.days\n\n#Can get rid of intermediate columns\ndf = df.drop([\"draft_date\", \"draft_year\", \"dob\"], axis = 1)\n\n#Adding columns for goals/g and points/g\ndf[\"gpg\"] = df[\"g\"] / df[\"gp\"]\ndf[\"apg\"] = df[\"a\"] / df[\"gp\"]\ndf\n\n\n\n\n\n\n\n\nyear\nplayer\ngp\ng\na\ntp\nheight_cm\nshoots\nnhl_gp\nweight_kg\nage_days\ngpg\napg\n\n\n\n\n0\n2019-2020\nMARCO ROSSI\n56\n39\n81\n120\n176\nL\n0\n87\n6855\n0.696429\n1.446429\n\n\n1\n2019-2020\nCOLE PERFETTI\n61\n37\n74\n111\n180\nL\n0\n84\n6755\n0.606557\n1.213115\n\n\n2\n2019-2020\nJACK QUINN\n62\n52\n37\n89\n185\nR\n0\n84\n6859\n0.838710\n0.596774\n\n\n3\n2019-2020\nQUINTON BYFIELD\n45\n32\n50\n82\n195\nL\n0\n102\n6525\n0.711111\n1.111111\n\n\n4\n2019-2020\nTYSON FOERSTER\n62\n36\n44\n80\n188\nR\n0\n97\n6738\n0.580645\n0.709677\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n111\n2024-2025\nHARRY NANSI\n67\n7\n16\n23\n190\nR\n0\n84\n6503\n0.104478\n0.238806\n\n\n112\n2024-2025\nJORDAN CHARRON\n48\n10\n11\n21\n185\nL\n0\n86\n6584\n0.208333\n0.229167\n\n\n113\n2024-2025\nMATTHEW HLACAR\n43\n7\n1\n8\n191\nL\n0\n91\n6970\n0.162791\n0.023256\n\n\n114\n2024-2025\nKIEREN DERVIN\n10\n1\n2\n3\n186\nL\n0\n83\n6666\n0.100000\n0.200000\n\n\n115\n2024-2025\nPARKER HOLMES\n21\n1\n0\n1\n193\nL\n0\n101\n6689\n0.047619\n0.000000\n\n\n\n\n116 rows × 13 columns\n\n\n\n\n#Storing Other Data used by app / report in s3 bucket:\nboard = board_s3(\"devakshah-stat468-models\", allow_pickle_read=True)\nboard.pin_write(df, name=\"prospects_2020_to_2025_data\", type=\"csv\")\n\nWriting pin:\nName: 'prospects_2020_to_2025_data'\nVersion: 20250810T234638Z-0844c\n\n\nMeta(title='prospects_2020_to_2025_data: a pinned 116 x 13 DataFrame', description=None, created='20250810T234638Z', pin_hash='0844c0688795d48e', file='prospects_2020_to_2025_data.csv', file_size=9946, type='csv', api_version=1, version=Version(created=datetime.datetime(2025, 8, 10, 23, 46, 38, 493497), hash='0844c0688795d48e'), tags=None, name='prospects_2020_to_2025_data', user={}, local={})"
  }
]