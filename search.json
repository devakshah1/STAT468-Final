[
  {
    "objectID": "final_project.html",
    "href": "final_project.html",
    "title": "STAT468 Final Project",
    "section": "",
    "text": "import TopDownHockey_Scraper.TopDownHockey_NHL_Scraper as tdhnhlscrape\nimport TopDownHockey_Scraper.TopDownHockey_EliteProspects_Scraper as tdhepscrape\nfrom nhlpy import NHLClient\n\nWelcome to the TopDownHockey NHL Scraper, built by Patrick Bacon.\nIf you enjoy the scraper and would like to support my work, or you have any comments, questions, or concerns, feel free to follow me on Twitter @TopDownHockey or reach out to me via email at patrick.s.bacon@gmail.com. Have fun!\nWelcome to the TopDownHockey EliteProspects Scraper, built by Patrick Bacon.\nThis scraper is built strictly for personal use. For commercial or professional use, please look into the EliteProspects API.\nIf you enjoy the scraper and would like to support my work, feel free to follow me on Twitter @TopDownHockey. Have fun!\n\n\n\nimport pandas as pd\nfrom datetime import date\nimport statsmodels.api as sm\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\n\n#Toggle to see if we want to rescrape, or just import prior scrape from existing file\nscrape = False\nfilename = \"regression_input.xlsx\"\n\n\n#Get OHL Player Data (stats, build, etc)\n\nif scrape == True:\n    #Can't include all years because of backend data issues in some years\n    #years = [\"2004-2005\", \"2006-2007\", \"2007-2008\", \"2008-2009\", \"2009-2010\", \"2010-2011\", \"2011-2012\", \"2012-2013\", \"2013-2014\", \n    #         \"2014-2015\", \"2015-2016\", \"2016-2017\", \"2017-2018\", \"2018-2019\"]\n\n    years = [\"2016-2017\", \"2013-2014\"]\n\n    aggregated_output = pd.DataFrame()\n\n    for year in years:\n        df = tdhepscrape.get_skaters((\"ohl\"), (year))[0:10]\n\n        #GET PLAYER INFO\n        info = tdhepscrape.get_player_information(df)\n\n        #GET RID OF D-MEN\n        df = df[~df['player'].str.contains(r'\\(([^)]*D[^)]*)\\)', regex=True)]\n\n        #GET RID OF PLAYER POSITIONS FROM NAMES\n        df['player'] = df['player'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n\n        #ADD YEAR TO DF\n        df.insert(0, \"year\", year)\n\n        #JOIN PLAYER BIO WITH STATS\n        year_output = pd.merge(df[[\"year\", \"player\", \"gp\", \"g\", \"a\", \"tp\"]], info[[\"player\", \"dob\", \"height\", \"weight\", \"shoots\"]], on='player', how='inner')\n\n        #ADD CURRENT YEAR PROSPECTS TO AGGREGATED DF\n        aggregated_output = pd.concat([aggregated_output, year_output])\n\n\n#Changing types of all columns, they are objects by default\nif scrape == True:\n    aggregated_output[\"gp\"] = aggregated_output[\"gp\"].replace(\"-\", 0)\n    aggregated_output[\"gp\"] = aggregated_output[\"gp\"].astype(int)\n    aggregated_output[\"g\"] = aggregated_output[\"g\"].replace(\"-\", 0)\n    aggregated_output[\"g\"] = aggregated_output[\"g\"].astype(int)\n    aggregated_output[\"a\"] = aggregated_output[\"a\"].replace(\"-\", 0)\n    aggregated_output[\"a\"] = aggregated_output[\"a\"].astype(int)\n    aggregated_output[\"tp\"] = aggregated_output[\"tp\"].replace(\"-\", 0)\n    aggregated_output[\"tp\"] = aggregated_output[\"tp\"].astype(int)\n\n\n#Get corresponding draft year for every season (oe. 2024-2025 would have a draft year of 2025)\n# note this isn't necessarily the players' draft year - it's just the draft year for the corresponding row's season\n\nif scrape == True:\n    aggregated_output[\"draft_year\"] = aggregated_output[\"year\"].str[5:]\n    aggregated_output[\"draft_year\"] = aggregated_output[\"draft_year\"].astype(int)\n    aggregated_output\n\n\n#Getting all NHL players drafted form 2005 - 2020\nif scrape == True:\n    years = list(range(2005, 2021))\n\n    draftyears = pd.DataFrame()\n\n    for year in years:\n        df_list = pd.read_html(f\"https://www.hockey-reference.com/draft/NHL_{year}_entry.html\", match=\"Round\")\n\n        players_drafted = df_list[0]\n\n        players_drafted\n\n        #Let's get rid of the top header that isnt really used\n        players_drafted.columns = players_drafted.columns.get_level_values(1)\n\n        players_drafted[\"draft_year\"] = year\n        players_drafted = players_drafted[[\"draft_year\", \"Player\"]]\n        players_drafted =players_drafted.rename(columns={\"draft_year\": \"player_draft_year\", \"Player\": \"player\"})\n        draftyears = pd.concat([draftyears, players_drafted])\n\n    draftyears        \n\n\n#Getting games played for all NHL players\nif scrape == True:\n    nhl_gp = pd.DataFrame()\n    pages = list(range(1, 80))\n\n    for page in pages:\n        df_list = pd.read_html(f\"https://www.eliteprospects.com/league/nhl/stats/all-time?page={page}\")\n        page_stats = df_list[2]\n        page_stats = page_stats[[\"Player\", \"GP\"]]\n        nhl_gp = pd.concat([nhl_gp, page_stats])\n\n    nhl_gp = nhl_gp.rename(columns={\"Player\": \"player\", \"GP\": \"nhl_gp\"})\n\n    #GET RID OF PLAYER POSITIONS FROM NAMES\n    nhl_gp['player'] = nhl_gp['player'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n\n    # Replacing NA and \"-\" values with 0\n    nhl_gp[\"nhl_gp\"] = nhl_gp[\"nhl_gp\"].fillna(0)\n    nhl_gp[\"nhl_gp\"] = nhl_gp[\"nhl_gp\"].replace(\"-\", 0)\n\n    nhl_gp\n\n\n#Join GP, draft year onto OHL player data\nif scrape == True:\n    #MAKE ALL PLAYER NAMES UPPERCASE (TO MAKE JOINING TABLES NON CASE SENSITIVE)\n    aggregated_output['player'] = aggregated_output['player'].str.upper()\n    draftyears['player'] = draftyears['player'].str.upper()\n    nhl_gp['player'] = nhl_gp['player'].str.upper()\n\n\n    #Filter ohl stats for only drafted players' draft year stats - \n    #This will get rid of a) undrafted players, and b) drafted players non-draft year stats\n    df = pd.merge(aggregated_output, draftyears, left_on=['player', 'draft_year'], right_on=['player', 'player_draft_year'], how='inner')\n\n    #Can get rid of one of the draft year columns - don't need both\n    df = df[[\"year\", \"player\", \"gp\", \"g\", \"a\", \"tp\", \"dob\", \"height\", \"weight\", \"shoots\", 'draft_year']]\n\n    #Join players' games played - if player gp not found, assume it to be 0.\n    df = pd.merge(df, nhl_gp, left_on=['player'], right_on=['player'], how='left')\n    df[\"nhl_gp\"] = df[\"nhl_gp\"].fillna(0)\n    df[\"nhl_gp\"] = df[\"nhl_gp\"].astype(int)\n\n    df.to_excel(filename, index=False)\n\n\n#SOME DATA CLEANING\n\ndf = pd.read_excel(filename)\n\n#Changing the weight to a numerical variate in kg\ndf[\"weight_kg\"] = df[\"weight\"].apply(lambda x: x[:x.find(\" \")])\ndf[\"weight_kg\"] = df[\"weight_kg\"].astype(int)\ndf.dtypes\n\n#Getting the age of the player at the time of draft (for simplicity, we will assume draft to be on June 30 for all years)\n\ndf[\"draft_date\"] = df[\"draft_year\"].astype(str) + '-06-30'\ndf[\"draft_date\"] = pd.to_datetime(df[\"draft_date\"])\ndf[\"dob\"] = pd.to_datetime(df[\"dob\"])\ndf[\"age_days\"] = (df[\"draft_date\"] - df[\"dob\"])\ndf[\"age_days\"] = df[\"age_days\"].dt.days\n\n#Can get rid of intermediate columns\ndf = df.drop([\"dob\", \"weight\", \"draft_year\", \"draft_date\"], axis = 1)\n\n#Adding columns for goals/g and points/g\ndf[\"gpg\"] = df[\"g\"] / df[\"gp\"]\ndf[\"ppg\"] = df[\"tp\"] / df[\"gp\"]\n\n#Create indicator variable to measure if the player has played at least 200 nhl games\ndf[\"Pr[GP &gt; 200]\"] = df[\"nhl_gp\"] &gt;= 200\ndf[\"Pr[GP &gt; 200]\"].astype(int)\n\n#Renaming height column to height_cm for clarity\ndf = df.rename(columns = {\"height\": \"height_cm\"})\ndf\n\n\n\n\n\n\n\n\nyear\nplayer\ngp\ng\na\ntp\nheight_cm\nshoots\nnhl_gp\nweight_kg\nage_days\ngpg\nppg\nPr[GP &gt; 200]\n\n\n\n\n0\n2006-2007\nPATRICK KANE\n58\n62\n83\n145\n178\nL\n1302\n80\n6797\n1.068966\n2.500000\nTrue\n\n\n1\n2006-2007\nSAM GAGNER\n53\n35\n83\n118\n180\nR\n1043\n89\n6533\n0.660377\n2.226415\nTrue\n\n\n2\n2006-2007\nBRETT MACLEAN\n68\n47\n53\n100\n188\nR\n18\n89\n6762\n0.691176\n1.470588\nFalse\n\n\n3\n2006-2007\nSTEFAN LEGEIN\n64\n43\n32\n75\n178\nR\n0\n77\n6792\n0.671875\n1.171875\nFalse\n\n\n4\n2006-2007\nZACK TORQUATO\n65\n30\n39\n69\n183\nR\n0\n88\n6596\n0.461538\n1.061538\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n283\n2019-2020\nDECLAN MCDONNELL\n63\n21\n21\n42\n178\nR\n0\n86\n6700\n0.333333\n0.666667\nFalse\n\n\n284\n2019-2020\nANTONIO STRANGES\n61\n19\n21\n40\n180\nL\n0\n84\n6720\n0.311475\n0.655738\nFalse\n\n\n285\n2019-2020\nTANNER DICKINSON\n64\n9\n31\n40\n183\nL\n0\n80\n6692\n0.140625\n0.625000\nFalse\n\n\n286\n2019-2020\nMARTIN CHROMIAK\n28\n11\n22\n33\n183\nR\n0\n86\n6524\n0.392857\n1.178571\nFalse\n\n\n287\n2019-2020\nJAN MYSAK\n22\n15\n10\n25\n180\nL\n0\n86\n6581\n0.681818\n1.136364\nFalse\n\n\n\n\n288 rows × 14 columns\n\n\n\n\n# X: predictors, y: binary response\ndf_regress = df[df[\"year\"].isin([\"2004-2005\", \"2006-2007\", \"2007-2008\", \"2008-2009\", \"2009-2010\", \n                                 \"2014-2015\", \"2015-2016\", \"2016-2017\", \"2017-2018\", \"2018-2019\"])]\nX = df_regress[['g', 'a', 'tp', 'height_cm', 'weight_kg', 'age_days', 'gpg', 'ppg']]\nX = sm.add_constant(X)  # adds intercept\ny = df_regress['Pr[GP &gt; 200]']\n\nmodel = sm.Logit(y, X)\nresult = model.fit()\n\nprint(result.summary())\n\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.488150\n         Iterations: 35\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:           Pr[GP &gt; 200]   No. Observations:                  167\nModel:                          Logit   Df Residuals:                      158\nMethod:                           MLE   Df Model:                            8\nDate:                Mon, 04 Aug 2025   Pseudo R-squ.:                  0.2244\nTime:                        20:14:31   Log-Likelihood:                -81.521\nconverged:                      False   LL-Null:                       -105.11\nCovariance Type:            nonrobust   LLR p-value:                 1.424e-07\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.7392     10.224     -0.072      0.942     -20.778      19.299\ng             -0.0991        nan        nan        nan         nan         nan\na              0.0749        nan        nan        nan         nan         nan\ntp            -0.0238        nan        nan        nan         nan         nan\nheight_cm      0.0524      0.047      1.109      0.267      -0.040       0.145\nweight_kg      0.0318      0.040      0.801      0.423      -0.046       0.110\nage_days      -0.0023      0.001     -2.140      0.032      -0.004      -0.000\ngpg            7.3258     22.332      0.328      0.743     -36.444      51.095\nppg            1.4341      9.650      0.149      0.882     -17.480      20.349\n==============================================================================\n\n\n/Users/devakshah/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\n\n#Testing model out\ndf_test = df[df[\"year\"].isin([\"2010-2011\", \"2011-2012\", \"2012-2013\", \"2013-2014\"])]\nX = df_test[['g', 'a', 'tp', 'height_cm', 'weight_kg', 'age_days', 'gpg', 'ppg']]\nX = sm.add_constant(X, has_constant='add')  # adds intercept\n\n#If regression yields a probability greater than or equal to 0.5, we will say it predicts the player will become a full-time NHL player\ndf_test[\"pred_prob\"] = result.predict(X) &gt;= 0.5\ndf_test\n\n\n\n\n\n\n\n\nyear\nplayer\ngp\ng\na\ntp\nheight_cm\nshoots\nnhl_gp\nweight_kg\nage_days\ngpg\nppg\nPr[GP &gt; 200]\npred_prob\n\n\n\n\n72\n2010-2011\nRYAN STROME\n65\n33\n73\n106\n185\nR\n864\n87\n6563\n0.507692\n1.630769\nTrue\nTrue\n\n\n73\n2010-2011\nSHANE PRINCE\n59\n25\n63\n88\n181\nL\n128\n88\n6800\n0.423729\n1.491525\nFalse\nTrue\n\n\n74\n2010-2011\nSTEFAN NOESEN\n68\n34\n43\n77\n185\nR\n444\n93\n6712\n0.500000\n1.132353\nTrue\nFalse\n\n\n75\n2010-2011\nANDY ANDREOFF\n66\n33\n42\n75\n185\nL\n188\n95\n7349\n0.500000\n1.136364\nFalse\nFalse\n\n\n76\n2010-2011\nMARK SCHEIFELE\n66\n22\n53\n75\n190\nR\n879\n94\n6681\n0.333333\n1.136364\nTrue\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n168\n2013-2014\nCRISTIANO DIGIACINTO\n50\n17\n11\n28\n183\nL\n0\n88\n6746\n0.340000\n0.560000\nFalse\nFalse\n\n\n169\n2013-2014\nJAKE EVANS\n57\n11\n7\n18\n183\nL\n350\n89\n7072\n0.192982\n0.315789\nTrue\nFalse\n\n\n170\n2013-2014\nJADEN LINDO\n40\n9\n9\n18\n188\nR\n0\n97\n6745\n0.225000\n0.450000\nFalse\nFalse\n\n\n171\n2013-2014\nCHRISTIAN DVORAK\n33\n6\n8\n14\n185\nL\n534\n91\n6723\n0.181818\n0.424242\nTrue\nFalse\n\n\n172\n2013-2014\nKYLE PETTIT\n53\n5\n5\n10\n193\nL\n0\n91\n6737\n0.094340\n0.188679\nFalse\nFalse\n\n\n\n\n101 rows × 15 columns\n\n\n\n\nTP = df_test[(df_test[\"Pr[GP &gt; 200]\"] == True) & (df_test[\"pred_prob\"] == True)].shape[0]\nprint(f\"True Positive: {TP}\")\n\nTN = df_test[(df_test[\"Pr[GP &gt; 200]\"] == False) & (df_test[\"pred_prob\"] == False)].shape[0]\nprint(f\"True Negative: {TN}\")\n\nFP = df_test[(df_test[\"Pr[GP &gt; 200]\"] == False) & (df_test[\"pred_prob\"] == True)].shape[0]\nprint(f\"False Positive: {FP}\")\n\nFN = df_test[(df_test[\"Pr[GP &gt; 200]\"] == True) & (df_test[\"pred_prob\"] == False)].shape[0]\nprint(f\"False Negative: {FN}\")\n\nsuccess_rate = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) &gt; 0 else 0\nprint(f\"Success Rate: {success_rate:.2%}\")\n\npositive_success_rate = TP / (TP + FP) if (TP + FP) &gt; 0 else 0\nprint(f\"Positive Success Rate: {positive_success_rate:.2%}\")\n\nnhler_success_rate = TP / (TP + FN) if (TP + FN) &gt; 0 else 0\nprint(f\"NHLer Success Rate: {nhler_success_rate:.2%}\")\n\nTrue Positive: 10\nTrue Negative: 59\nFalse Positive: 4\nFalse Negative: 28\nSuccess Rate: 68.32%\nPositive Success Rate: 71.43%\nNHLer Success Rate: 26.32%\n\n\n\nfrom vetiver.handlers.base import BaseHandler\nimport statsmodels.api as sm\n\n#Need to create a custom handler for statsmodels Logit models, as the default handler cannot handle this type of model\nclass StatsmodelsLogitHandler(BaseHandler):\n    def __init__(self, model, prototype_data):\n        super().__init__(model, prototype_data)\n\n    @staticmethod\n    def model_type():\n        return \"statsmodels_logit\"\n\n    pip_name = \"statsmodels\"\n\n    def handler_predict(self, input_data, check_prototype: bool):\n        \"\"\"\n        Make predictions using a fitted statsmodels Logit model.\n\n        Parameters\n        ----------\n        input_data:\n            New data (e.g., from API)\n        check_prototype: bool\n            Whether to check data shape\n\n        Returns\n        -------\n        Prediction array from model.predict\n        \"\"\"\n        # Add constant to match model spec\n        input_data_const = sm.add_constant(input_data, has_constant='add')\n        prediction = self.model.predict(input_data_const)\n        return prediction\n\n\nfrom pins import board_s3\nfrom vetiver import vetiver_pin_write\nfrom vetiver import VetiverModel\n\n#Store the model in an S3 bucket:\n\nboard = board_s3(\"devakshah-stat468-models\", allow_pickle_read=True)\n\ncustom_handler = StatsmodelsLogitHandler(result, prototype_data=X)\nvetiver_model = VetiverModel(custom_handler, model_name=\"my_logit_model\", description=\"Logistic regression\", handler=custom_handler)\n\nvetiver_pin_write(board, vetiver_model)\n\nModel Cards provide a framework for transparent, responsible reporting. \n Use the vetiver `.qmd` Quarto template as a place to start, \n with vetiver.model_card()\n('The hash of pin \"my_logit_model\" has not changed. Your pin will not be stored.',)\n\n\n\n#Storing Other Data used by app / report in s3 bucket:\nboard = board_s3(\"devakshah-stat468-models\", allow_pickle_read=True)\nboard.pin_write(df, name=\"regression_input_data\", type=\"csv\")\n\nWriting pin:\nName: 'regression_input_data'\nVersion: 20250804T201549Z-c3a37\n\n\nMeta(title='regression_input_data: a pinned 288 x 14 DataFrame', description=None, created='20250804T201549Z', pin_hash='c3a374df234b2935', file='regression_input_data.csv', file_size=26991, type='csv', api_version=1, version=Version(created=datetime.datetime(2025, 8, 4, 20, 15, 49, 442041), hash='c3a374df234b2935'), tags=None, name='regression_input_data', user={}, local={})"
  },
  {
    "objectID": "final_project_report.html",
    "href": "final_project_report.html",
    "title": "STAT468 Final Project",
    "section": "",
    "text": "CHAPTER 1: IMPORT\nThe first step in our project is to import the necessary libraries and data:\n\nimport TopDownHockey_Scraper.TopDownHockey_NHL_Scraper as tdhnhlscrape\nimport TopDownHockey_Scraper.TopDownHockey_EliteProspects_Scraper as tdhepscrape\nfrom nhlpy import NHLClient\n\n\nimport pandas as pd\nfrom datetime import date\nimport statsmodels.api as sm\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\nWe will create a toggle to see if we want to rescrape the data or just import the prior scrape from an existing file. If scrape is set to True, we will scrape the data; otherwise, we will read from the existing file.\n\nscrape = True\n\n#This will be the file name where data is stored in our git repo\nfilename = \"regression_input.xlsx\"\n\nFirst, we loop through a bunch of seasons and scrape player OHL stats and builds. Not that we get rid of player positions so that we can join with player builds using names as index. While this is mixing some of steps 2&3 (Tidy&Transform), it’s easier to do it here since we want to get the player builds at the time of the draft, and it would be confusing to do the join later on, when we’re outside the loop and the stats&builds may not correspond to one another since they could be from seperate years.\n\n#Get OHL Player Data (stats, build, etc)\n\nif scrape == True:\n    #Can't include all years because of backend data issues in some years\n    #years = [\"2004-2005\", \"2006-2007\", \"2007-2008\", \"2008-2009\", \"2009-2010\", \"2010-2011\", \"2011-2012\", \"2012-2013\", \"2013-2014\", \n    #         \"2014-2015\", \"2015-2016\", \"2016-2017\", \"2017-2018\", \"2018-2019\"]\n\n    years = [\"2016-2017\", \"2013-2014\"]\n\n    aggregated_output = pd.DataFrame()\n\n    for year in years:\n        df = tdhepscrape.get_skaters((\"ohl\"), (year))[0:50]\n\n        #GET PLAYER INFO\n        info = tdhepscrape.get_player_information(df)\n\n        #GET RID OF PLAYER POSITIONS FROM NAMES\n        df['player'] = df['player'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n\n        #ADD YEAR TO DF\n        df.insert(0, \"year\", year)\n\n        #JOIN PLAYER BIO WITH STATS\n        year_output = pd.merge(df[[\"year\", \"player\", \"gp\", \"g\", \"a\", \"tp\"]], info[[\"player\", \"dob\", \"height\", \"weight\", \"shoots\"]], on='player', how='inner')\n\n        #ADD CURRENT YEAR PROSPECTS TO AGGREGATED DF\n        aggregated_output = pd.concat([aggregated_output, year_output])\n\nYour scrape request is skater data from the following league:\nohl\nIn the following season:\n2016-2017\nBeginning scrape of ohl skater data from 2016-2017.\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[121], line 13\n     10 aggregated_output = pd.DataFrame()\n     12 for year in years:\n---&gt; 13     df = tdhepscrape.get_skaters((\"ohl\"), (year))[0:50]\n     15     #GET PLAYER INFO\n     16     info = tdhepscrape.get_player_information(df)\n\nFile ~/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/TopDownHockey_Scraper/TopDownHockey_EliteProspects_Scraper.py:682, in get_skaters(leagues, seasons)\n    680 print(\"In the following season:\")\n    681 print(season_string)\n--&gt; 682 leaguesall = get_league_skater_boxcars(leagues, seasons)\n    683 print(\"Completed scraping skater data from the following league:\")\n    684 print(str(leagues))\n\nFile ~/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/TopDownHockey_Scraper/TopDownHockey_EliteProspects_Scraper.py:430, in get_league_skater_boxcars(league, seasons)\n    427 output = pd.DataFrame()\n    429 if type(seasons) == str:\n--&gt; 430     single = getskaters(league, seasons)\n    431     output = output._append(single)\n    432     print(\"Scraping \" + league + \" data is complete. You scraped skater data from \" + seasons + \".\")\n\nFile ~/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/TopDownHockey_Scraper/TopDownHockey_EliteProspects_Scraper.py:69, in getskaters(league, year)\n     66 else:\n     68     for i in range(1,99):\n---&gt; 69         page = requests.get(url+str(i), timeout = 500) \n     70         page_string = str(page)\n     72         while page_string == '&lt;Response [403]&gt;':\n\nFile ~/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/requests/api.py:73, in get(url, params, **kwargs)\n     62 def get(url, params=None, **kwargs):\n     63     r\"\"\"Sends a GET request.\n     64 \n     65     :param url: URL for the new :class:`Request` object.\n   (...)     70     :rtype: requests.Response\n     71     \"\"\"\n---&gt; 73     return request(\"get\", url, params=params, **kwargs)\n\nFile ~/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/requests/api.py:59, in request(method, url, **kwargs)\n     55 # By using the 'with' statement we are sure the session is closed, thus we\n     56 # avoid leaving sockets open which can trigger a ResourceWarning in some\n     57 # cases, and look like a memory leak in others.\n     58 with sessions.Session() as session:\n---&gt; 59     return session.request(method=method, url=url, **kwargs)\n\nFile ~/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\n    584 send_kwargs = {\n    585     \"timeout\": timeout,\n    586     \"allow_redirects\": allow_redirects,\n    587 }\n    588 send_kwargs.update(settings)\n--&gt; 589 resp = self.send(prep, **send_kwargs)\n    591 return resp\n\nFile ~/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/requests/sessions.py:703, in Session.send(self, request, **kwargs)\n    700 start = preferred_clock()\n    702 # Send the request\n--&gt; 703 r = adapter.send(request, **kwargs)\n    705 # Total elapsed time of the request (approximately)\n    706 elapsed = preferred_clock() - start\n\nFile ~/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/requests/adapters.py:667, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies)\n    664     timeout = TimeoutSauce(connect=timeout, read=timeout)\n    666 try:\n--&gt; 667     resp = conn.urlopen(\n    668         method=request.method,\n    669         url=url,\n    670         body=request.body,\n    671         headers=request.headers,\n    672         redirect=False,\n    673         assert_same_host=False,\n    674         preload_content=False,\n    675         decode_content=False,\n    676         retries=self.max_retries,\n    677         timeout=timeout,\n    678         chunked=chunked,\n    679     )\n    681 except (ProtocolError, OSError) as err:\n    682     raise ConnectionError(err, request=request)\n\nFile ~/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/urllib3/connectionpool.py:787, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\n    784 response_conn = conn if not release_conn else None\n    786 # Make the request on the HTTPConnection object\n--&gt; 787 response = self._make_request(\n    788     conn,\n    789     method,\n    790     url,\n    791     timeout=timeout_obj,\n    792     body=body,\n    793     headers=headers,\n    794     chunked=chunked,\n    795     retries=retries,\n    796     response_conn=response_conn,\n    797     preload_content=preload_content,\n    798     decode_content=decode_content,\n    799     **response_kw,\n    800 )\n    802 # Everything went great!\n    803 clean_exit = True\n\nFile ~/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/urllib3/connectionpool.py:534, in HTTPConnectionPool._make_request(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\n    532 # Receive the response from the server\n    533 try:\n--&gt; 534     response = conn.getresponse()\n    535 except (BaseSSLError, OSError) as e:\n    536     self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n\nFile ~/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/urllib3/connection.py:565, in HTTPConnection.getresponse(self)\n    562 _shutdown = getattr(self.sock, \"shutdown\", None)\n    564 # Get the response from http.client.HTTPConnection\n--&gt; 565 httplib_response = super().getresponse()\n    567 try:\n    568     assert_header_parsing(httplib_response.msg)\n\nFile /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1430, in HTTPConnection.getresponse(self)\n   1428 try:\n   1429     try:\n-&gt; 1430         response.begin()\n   1431     except ConnectionError:\n   1432         self.close()\n\nFile /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:331, in HTTPResponse.begin(self)\n    329 # read until we get a non-100 response\n    330 while True:\n--&gt; 331     version, status, reason = self._read_status()\n    332     if status != CONTINUE:\n    333         break\n\nFile /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:292, in HTTPResponse._read_status(self)\n    291 def _read_status(self):\n--&gt; 292     line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n    293     if len(line) &gt; _MAXLINE:\n    294         raise LineTooLong(\"status line\")\n\nFile /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719, in SocketIO.readinto(self, b)\n    717     raise OSError(\"cannot read from timed out object\")\n    718 try:\n--&gt; 719     return self._sock.recv_into(b)\n    720 except timeout:\n    721     self._timeout_occurred = True\n\nFile /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304, in SSLSocket.recv_into(self, buffer, nbytes, flags)\n   1300     if flags != 0:\n   1301         raise ValueError(\n   1302           \"non-zero flags not allowed in calls to recv_into() on %s\" %\n   1303           self.__class__)\n-&gt; 1304     return self.read(nbytes, buffer)\n   1305 else:\n   1306     return super().recv_into(buffer, nbytes, flags)\n\nFile /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138, in SSLSocket.read(self, len, buffer)\n   1136 try:\n   1137     if buffer is not None:\n-&gt; 1138         return self._sslobj.read(len, buffer)\n   1139     else:\n   1140         return self._sslobj.read(len)\n\nKeyboardInterrupt: \n\n\n\nNext, we print the draftyears of all NHL players drafted between 2005 and 2020\n\nif scrape == True:\n    years = list(range(2005, 2021))\n\n    draftyears = pd.DataFrame()\n\n    for year in years:\n        df_list = pd.read_html(f\"https://www.hockey-reference.com/draft/NHL_{year}_entry.html\", match=\"Round\")\n\n        players_drafted = df_list[0]\n\n        players_drafted\n\n        #Let's get rid of the top header that isnt really used\n        players_drafted.columns = players_drafted.columns.get_level_values(1)\n\n        players_drafted[\"draft_year\"] = year\n        players_drafted = players_drafted[[\"draft_year\", \"Player\"]]\n        players_drafted =players_drafted.rename(columns={\"draft_year\": \"player_draft_year\", \"Player\": \"player\"})\n        draftyears = pd.concat([draftyears, players_drafted])\n\n    print(draftyears)        \n\n     player_draft_year            player\n0                 2005     Sidney Crosby\n1                 2005        Bobby Ryan\n2                 2005      Jack Johnson\n3                 2005    Benoit Pouliot\n4                 2005       Carey Price\n..                 ...               ...\n223               2020     Ryan Tverberg\n224               2020   Henrik Tikkanen\n225               2020    Maxim Marushev\n226               2020     Jakub Konecny\n227               2020  Declan McDonnell\n\n[3609 rows x 2 columns]\n\n\nFinally, we get the games played for all NHL players to have played at least 1 game in the NHL. We will use this to calculate the probability of a player playing more than 200 games in the NHL.\n\n#Getting games played for all NHL players\nif scrape == True:\n    nhl_gp = pd.DataFrame()\n    pages = list(range(1, 80))\n\n    for page in pages:\n        df_list = pd.read_html(f\"https://www.eliteprospects.com/league/nhl/stats/all-time?page={page}\")\n        page_stats = df_list[2]\n        page_stats = page_stats[[\"Player\", \"GP\"]]\n        nhl_gp = pd.concat([nhl_gp, page_stats])\n\n    nhl_gp = nhl_gp.rename(columns={\"Player\": \"player\", \"GP\": \"nhl_gp\"})\n\n    #GET RID OF PLAYER POSITIONS FROM NAMES\n    nhl_gp['player'] = nhl_gp['player'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n\n    # Replacing NA and \"-\" values with 0\n    nhl_gp[\"nhl_gp\"] = nhl_gp[\"nhl_gp\"].fillna(0)\n    nhl_gp[\"nhl_gp\"] = nhl_gp[\"nhl_gp\"].replace(\"-\", 0)\n\n    nhl_gp\n\nCHAPTER 2: TIDY\nWe first get rid of defencemen from our dataset, since we are only concerned with forwards.\n\nif scrape == True:\n    aggregated_output = aggregated_output[~aggregated_output['player'].str.contains(r'\\(([^)]*D[^)]*)\\)', regex=True)]\n\nNext, we change the type of some columns - they are objects by default, we need them to be integeres to regress on them later on. We also replace “-” values with 0 for regression purposes, since “-” means that the player did not play in that season, and we want to treat that as 0 games played.\n\nif scrape == True:\n    aggregated_output[\"gp\"] = aggregated_output[\"gp\"].replace(\"-\", 0)\n    aggregated_output[\"gp\"] = aggregated_output[\"gp\"].astype(int)\n    aggregated_output[\"g\"] = aggregated_output[\"g\"].replace(\"-\", 0)\n    aggregated_output[\"g\"] = aggregated_output[\"g\"].astype(int)\n    aggregated_output[\"a\"] = aggregated_output[\"a\"].replace(\"-\", 0)\n    aggregated_output[\"a\"] = aggregated_output[\"a\"].astype(int)\n    aggregated_output[\"tp\"] = aggregated_output[\"tp\"].replace(\"-\", 0)\n    aggregated_output[\"tp\"] = aggregated_output[\"tp\"].astype(int)\n\nGet corresponding draft year for each row in the dataset (oe. season 2024-2025 would have a draft year of 2025).\nNote this isn’t necessarily the players’ draft year - it’s just the draft year for the corresponding row’s season. We will use this to only keep rows where the draft year is the same as the player’s actual draft year.\n\nif scrape == True:\n    aggregated_output[\"draft_year\"] = aggregated_output[\"year\"].str[5:]\n    aggregated_output[\"draft_year\"] = aggregated_output[\"draft_year\"].astype(int)\n    aggregated_output\n\nJoin games played and draft year onto each row of the OHL player data dataset.\n\n#Join GP, draft year onto OHL player data\nif scrape == True:\n    #MAKE ALL PLAYER NAMES UPPERCASE (TO MAKE JOINING TABLES NON CASE SENSITIVE)\n    aggregated_output['player'] = aggregated_output['player'].str.upper()\n    draftyears['player'] = draftyears['player'].str.upper()\n    nhl_gp['player'] = nhl_gp['player'].str.upper()\n\n\n    #Filter ohl stats for only drafted players' draft year stats - \n    #This will get rid of a) undrafted players, and b) drafted players non-draft year stats\n    df = pd.merge(aggregated_output, draftyears, left_on=['player', 'draft_year'], right_on=['player', 'player_draft_year'], how='inner')\n\n    #Can get rid of one of the draft year columns - don't need both\n    df = df[[\"year\", \"player\", \"gp\", \"g\", \"a\", \"tp\", \"dob\", \"height\", \"weight\", \"shoots\", 'draft_year']]\n\n    #Join players' games played - if player gp not found, assume it to be 0.\n    df = pd.merge(df, nhl_gp, left_on=['player'], right_on=['player'], how='left')\n    df[\"nhl_gp\"] = df[\"nhl_gp\"].fillna(0)\n    df[\"nhl_gp\"] = df[\"nhl_gp\"].astype(int)\n\n    df.to_excel(filename, index=False)\n\nFinally, some miscallaneous tidying. Refer to comments for more information.\n\ndf = pd.read_excel(filename)\n\n#Changing the weight to a numerical variate in kg\ndf[\"weight_kg\"] = df[\"weight\"].apply(lambda x: x[:x.find(\" \")])\ndf[\"weight_kg\"] = df[\"weight_kg\"].astype(int)\ndf.dtypes\n\n#Can get rid of intermediate columns\ndf = df.drop([\"weight\"], axis = 1)\n\n#Renaming height column to height_cm for clarity\ndf = df.rename(columns = {\"height\": \"height_cm\"})\ndf\n\n\n\n\n\n\n\n\nyear\nplayer\ngp\ng\na\ntp\ndob\nheight_cm\nshoots\ndraft_year\nnhl_gp\nweight_kg\n\n\n\n\n0\n2016-2017\nPETRUS PALMU\n62\n40\n58\n98\n1997-07-16\n168\nL\n2017\n0\n78\n\n\n1\n2016-2017\nNICK SUZUKI\n65\n45\n51\n96\n1999-08-10\n180\nR\n2017\n455\n94\n\n\n2\n2016-2017\nJASON ROBERTSON\n68\n42\n39\n81\n1999-07-22\n191\nL\n2017\n374\n91\n\n\n3\n2016-2017\nOWEN TIPPETT\n60\n44\n31\n75\n1999-02-16\n185\nR\n2017\n347\n95\n\n\n4\n2016-2017\nJONAH GADJOVICH\n60\n46\n28\n74\n1998-10-12\n191\nL\n2017\n160\n96\n\n\n5\n2016-2017\nROBERT THOMAS\n66\n16\n50\n66\n1999-07-02\n183\nR\n2017\n466\n85\n\n\n6\n2016-2017\nDRAKE RYMSHA\n65\n35\n27\n62\n1998-08-06\n183\nR\n2017\n1\n91\n\n\n7\n2016-2017\nMATTHEW STROME\n66\n34\n28\n62\n1999-01-06\n193\nL\n2017\n0\n93\n\n\n8\n2016-2017\nMORGAN FROST\n67\n20\n42\n62\n1999-05-14\n182\nL\n2017\n310\n87\n\n\n9\n2016-2017\nGABRIEL VILARDI\n49\n29\n32\n61\n1999-08-16\n191\nR\n2017\n270\n98\n\n\n10\n2016-2017\nCONOR TIMMINS\n67\n7\n54\n61\n1998-09-18\n188\nR\n2017\n159\n97\n\n\n11\n2013-2014\nMICHAEL DAL COLLE\n67\n39\n56\n95\n1996-06-20\n191\nL\n2014\n112\n88\n\n\n12\n2013-2014\nBRENDAN PERLINI\n58\n34\n37\n71\n1996-04-27\n192\nL\n2014\n262\n96\n\n\n13\n2013-2014\nSPENCER WATSON\n65\n33\n35\n68\n1996-04-25\n178\nR\n2014\n0\n77\n\n\n14\n2013-2014\nJARED MCCANN\n64\n27\n35\n62\n1996-05-31\n185\nL\n2014\n668\n84\n\n\n15\n2013-2014\nERIC CORNEL\n68\n25\n37\n62\n1996-04-11\n188\nR\n2014\n0\n90\n\n\n\n\n\n\n\nSTEP 3: TRANSFORM\nCreating some new stats (age at draft, goals per game, points per game, and an indicator variable for whether the player has played at least 200 NHL games). These will all be used in the regression later on.\n\n#Getting the age of the player at the time of draft (for simplicity, we will assume draft to be on June 30 for all years)\ndf[\"draft_date\"] = df[\"draft_year\"].astype(str) + '-06-30'\ndf[\"draft_date\"] = pd.to_datetime(df[\"draft_date\"])\ndf[\"dob\"] = pd.to_datetime(df[\"dob\"])\ndf[\"age_days\"] = (df[\"draft_date\"] - df[\"dob\"])\ndf[\"age_days\"] = df[\"age_days\"].dt.days\n\n#Can get rid of intermediate columns\ndf = df.drop([\"draft_date\", \"draft_year\", \"dob\"], axis = 1)\n\n#Adding columns for goals/g and points/g\ndf[\"gpg\"] = df[\"g\"] / df[\"gp\"]\ndf[\"ppg\"] = df[\"tp\"] / df[\"gp\"]\n\n#Create indicator variable to measure if the player has played at least 200 nhl games\ndf[\"Pr[GP &gt; 200]\"] = df[\"nhl_gp\"] &gt;= 200\ndf[\"Pr[GP &gt; 200]\"] = df[\"Pr[GP &gt; 200]\"].astype(int)\n\ndf\n\n\n\n\n\n\n\n\nyear\nplayer\ngp\ng\na\ntp\nheight_cm\nshoots\nnhl_gp\nweight_kg\nage_days\ngpg\nppg\nPr[GP &gt; 200]\n\n\n\n\n0\n2016-2017\nPETRUS PALMU\n62\n40\n58\n98\n168\nL\n0\n78\n7289\n0.645161\n1.580645\n0\n\n\n1\n2016-2017\nNICK SUZUKI\n65\n45\n51\n96\n180\nR\n455\n94\n6534\n0.692308\n1.476923\n1\n\n\n2\n2016-2017\nJASON ROBERTSON\n68\n42\n39\n81\n191\nL\n374\n91\n6553\n0.617647\n1.191176\n1\n\n\n3\n2016-2017\nOWEN TIPPETT\n60\n44\n31\n75\n185\nR\n347\n95\n6709\n0.733333\n1.250000\n1\n\n\n4\n2016-2017\nJONAH GADJOVICH\n60\n46\n28\n74\n191\nL\n160\n96\n6836\n0.766667\n1.233333\n0\n\n\n5\n2016-2017\nROBERT THOMAS\n66\n16\n50\n66\n183\nR\n466\n85\n6573\n0.242424\n1.000000\n1\n\n\n6\n2016-2017\nDRAKE RYMSHA\n65\n35\n27\n62\n183\nR\n1\n91\n6903\n0.538462\n0.953846\n0\n\n\n7\n2016-2017\nMATTHEW STROME\n66\n34\n28\n62\n193\nL\n0\n93\n6750\n0.515152\n0.939394\n0\n\n\n8\n2016-2017\nMORGAN FROST\n67\n20\n42\n62\n182\nL\n310\n87\n6622\n0.298507\n0.925373\n1\n\n\n9\n2016-2017\nGABRIEL VILARDI\n49\n29\n32\n61\n191\nR\n270\n98\n6528\n0.591837\n1.244898\n1\n\n\n10\n2016-2017\nCONOR TIMMINS\n67\n7\n54\n61\n188\nR\n159\n97\n6860\n0.104478\n0.910448\n0\n\n\n11\n2013-2014\nMICHAEL DAL COLLE\n67\n39\n56\n95\n191\nL\n112\n88\n6584\n0.582090\n1.417910\n0\n\n\n12\n2013-2014\nBRENDAN PERLINI\n58\n34\n37\n71\n192\nL\n262\n96\n6638\n0.586207\n1.224138\n1\n\n\n13\n2013-2014\nSPENCER WATSON\n65\n33\n35\n68\n178\nR\n0\n77\n6640\n0.507692\n1.046154\n0\n\n\n14\n2013-2014\nJARED MCCANN\n64\n27\n35\n62\n185\nL\n668\n84\n6604\n0.421875\n0.968750\n1\n\n\n15\n2013-2014\nERIC CORNEL\n68\n25\n37\n62\n188\nR\n0\n90\n6654\n0.367647\n0.911765\n0\n\n\n\n\n\n\n\nSTEP 4: VISUALIZE\nSTEP 5: MODEL\nWe are going to create a logistic regression model. Logsitic regression is used since we are regressing for a probability, which needs to be bounded between 0 and 1. The model will aim to predict the probability a player becoming an everyday NHL player (play 200 NHL games or more):\n\n# X: predictors, y: binary response\ndf_regress = df[df[\"year\"].isin([\"2004-2005\", \"2006-2007\", \"2007-2008\", \"2008-2009\", \"2009-2010\", \n                                 \"2014-2015\", \"2015-2016\", \"2016-2017\", \"2017-2018\", \"2018-2019\"])]\nX = df_regress[['tp', 'height_cm', 'weight_kg', 'age_days', 'gpg', 'ppg']]\nX = sm.add_constant(X)  # adds intercept\ny = df_regress['Pr[GP &gt; 200]']\n\nmodel = sm.Logit(y, X)\nresult = model.fit()\n\nprint(result.summary())\n\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.000000\n         Iterations: 35\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:           Pr[GP &gt; 200]   No. Observations:                   11\nModel:                          Logit   Df Residuals:                        4\nMethod:                           MLE   Df Model:                            6\nDate:                Tue, 05 Aug 2025   Pseudo R-squ.:                   1.000\nTime:                        00:39:50   Log-Likelihood:            -3.9237e-09\nconverged:                      False   LL-Null:                       -7.5791\nCovariance Type:            nonrobust   LLR p-value:                   0.01906\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       3675.8803        nan        nan        nan         nan         nan\ntp             2.2330        nan        nan        nan         nan         nan\nheight_cm    -12.0059   6.89e+09  -1.74e-09      1.000   -1.35e+10    1.35e+10\nweight_kg     11.1335   3.02e+09   3.68e-09      1.000   -5.92e+09    5.92e+09\nage_days      -0.3737        nan        nan        nan         nan         nan\ngpg           33.7468        nan        nan        nan         nan         nan\nppg         -104.6696    3.4e+10  -3.07e-09      1.000   -6.67e+10    6.67e+10\n==============================================================================\n\nComplete Separation: The results show that there iscomplete separation or perfect prediction.\nIn this case the Maximum Likelihood Estimator does not exist and the parameters\nare not identified.\n\n\n/Users/devakshah/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n  warnings.warn(msg, category=PerfectSeparationWarning)\n/Users/devakshah/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n  warnings.warn(msg, category=PerfectSeparationWarning)\n/Users/devakshah/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n  warnings.warn(msg, category=PerfectSeparationWarning)\n/Users/devakshah/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n  warnings.warn(msg, category=PerfectSeparationWarning)\n/Users/devakshah/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n  warnings.warn(msg, category=PerfectSeparationWarning)\n/Users/devakshah/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n  warnings.warn(msg, category=PerfectSeparationWarning)\n/Users/devakshah/Downloads/STAT468/STAT468 Final/venv/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\nThe p-values tell us that …\nSTEP 6: COMMUNICATE\nLet’s first look at if there is multicollinearity in our data:\n\nprint(X.corr())\n\n           const        tp  height_cm  weight_kg  age_days       gpg       ppg\nconst        NaN       NaN        NaN        NaN       NaN       NaN       NaN\ntp           NaN  1.000000  -0.618148  -0.401491  0.292035  0.565618  0.895680\nheight_cm    NaN -0.618148   1.000000   0.784210 -0.572902 -0.038544 -0.525288\nweight_kg    NaN -0.401491   0.784210   1.000000 -0.490385  0.117542 -0.232743\nage_days     NaN  0.292035  -0.572902  -0.490385  1.000000  0.086337  0.256884\ngpg          NaN  0.565618  -0.038544   0.117542  0.086337  1.000000  0.705083\nppg          NaN  0.895680  -0.525288  -0.232743  0.256884  0.705083  1.000000\n\n\nNow, we will test out our model on a test set. We will use the model to predict the probability of a player becoming an everyday NHL player (playing 200 or more NHL games). We will do so on seasons we did not train our model on.\n\n#Testing model out\ndf_test = df[df[\"year\"].isin([\"2010-2011\", \"2011-2012\", \"2012-2013\", \"2013-2014\"])]\nX = df_test[['tp', 'height_cm', 'weight_kg', 'age_days', 'gpg', 'ppg']]\nX = sm.add_constant(X, has_constant='add')  # adds intercept\n\n#If regression yields a probability greater than or equal to 0.5, we will say it predicts the player will become a full-time NHL player\ndf_test[\"pred_prob\"] = result.predict(X) &gt;= 0.5\ndf_test\n\n\n\n\n\n\n\n\nyear\nplayer\ngp\ng\na\ntp\nheight_cm\nshoots\nnhl_gp\nweight_kg\nage_days\ngpg\nppg\nPr[GP &gt; 200]\npred_prob\n\n\n\n\n11\n2013-2014\nMICHAEL DAL COLLE\n67\n39\n56\n95\n191\nL\n112\n88\n6584\n0.582090\n1.417910\n0\nFalse\n\n\n12\n2013-2014\nBRENDAN PERLINI\n58\n34\n37\n71\n192\nL\n262\n96\n6638\n0.586207\n1.224138\n1\nTrue\n\n\n13\n2013-2014\nSPENCER WATSON\n65\n33\n35\n68\n178\nR\n0\n77\n6640\n0.507692\n1.046154\n0\nFalse\n\n\n14\n2013-2014\nJARED MCCANN\n64\n27\n35\n62\n185\nL\n668\n84\n6604\n0.421875\n0.968750\n1\nFalse\n\n\n15\n2013-2014\nERIC CORNEL\n68\n25\n37\n62\n188\nR\n0\n90\n6654\n0.367647\n0.911765\n0\nFalse\n\n\n\n\n\n\n\nNext, we will look at model accuracy. We will use TP, TN, FP, FN to calculate accuracy.\nFinally, we will look at the ROC curve and AUC to evaluate the model’s performance.\n\nTP = df_test[(df_test[\"Pr[GP &gt; 200]\"] == True) & (df_test[\"pred_prob\"] == True)].shape[0]\nprint(f\"True Positive: {TP}\")\n\nTN = df_test[(df_test[\"Pr[GP &gt; 200]\"] == False) & (df_test[\"pred_prob\"] == False)].shape[0]\nprint(f\"True Negative: {TN}\")\n\nFP = df_test[(df_test[\"Pr[GP &gt; 200]\"] == False) & (df_test[\"pred_prob\"] == True)].shape[0]\nprint(f\"False Positive: {FP}\")\n\nFN = df_test[(df_test[\"Pr[GP &gt; 200]\"] == True) & (df_test[\"pred_prob\"] == False)].shape[0]\nprint(f\"False Negative: {FN}\")\n\naccuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) &gt; 0 else 0\nprint(f\"Accuracy: {accuracy:.2%}\")\n\ntrue_positive_rate = TP / (TP + FN) if (TP + FN) &gt; 0 else 0\nprint(f\"True Positive Rate: {true_positive_rate:.2%}\")\n\ntrue_negative_rate = FP / (FP + TN) if (FP + TN) &gt; 0 else 0\nprint(f\"True Negative Rate: {true_negative_rate:.2%}\")\n\nTrue Positive: 1\nTrue Negative: 3\nFalse Positive: 0\nFalse Negative: 1\nAccuracy: 80.00%\nTrue Positive Rate: 50.00%\nTrue Negative Rate: 0.00%\n\n\nWe will also use ROC curve to look at the TPR, FPR of different thresholds. We can calculate AUR from this to measure accuracy of model considering all thresholds (AUS = 1 means perfect model, AUS = 0.5 means random guessing).:\n\nfrom sklearn.metrics import roc_curve, auc\n\nimport matplotlib.pyplot as plt\n\n# True labels and predicted probabilities\ny_true = df_test[\"Pr[GP &gt; 200]\"]\ny_score = result.predict(X)\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_true, y_score)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\nSTORE MODEL IN AWS S3 BUCKET\nThe code below will store the model in an AWS S3 bucket. I had to make a custom handler, since vetiver cannot handle sm.Logit models by default.\n\nfrom vetiver.handlers.base import BaseHandler\nimport statsmodels.api as sm\n\n#Need to create a custom handler for statsmodels Logit models, as the default handler cannot handle this type of model\nclass StatsmodelsLogitHandler(BaseHandler):\n    def __init__(self, model, prototype_data):\n        super().__init__(model, prototype_data)\n\n    @staticmethod\n    def model_type():\n        return \"statsmodels_logit\"\n\n    pip_name = \"statsmodels\"\n\n    def handler_predict(self, input_data, check_prototype: bool):\n        \"\"\"\n        Make predictions using a fitted statsmodels Logit model.\n\n        Parameters\n        ----------\n        input_data:\n            New data (e.g., from API)\n        check_prototype: bool\n            Whether to check data shape\n\n        Returns\n        -------\n        Prediction array from model.predict\n        \"\"\"\n        # Add constant to match model spec\n        input_data_const = sm.add_constant(input_data, has_constant='add')\n        prediction = self.model.predict(input_data_const)\n        return prediction\n\n\nfrom pins import board_s3\nfrom vetiver import vetiver_pin_write\nfrom vetiver import VetiverModel\n\n#Store the model in an S3 bucket:\n\nboard = board_s3(\"devakshah-stat468-models\", allow_pickle_read=True)\n\ncustom_handler = StatsmodelsLogitHandler(result, prototype_data=X)\nvetiver_model = VetiverModel(custom_handler, model_name=\"my_logit_model\", description=\"Logistic regression\", handler=custom_handler)\n\nvetiver_pin_write(board, vetiver_model)\n\nModel Cards provide a framework for transparent, responsible reporting. \n Use the vetiver `.qmd` Quarto template as a place to start, \n with vetiver.model_card()\n('The hash of pin \"my_logit_model\" has not changed. Your pin will not be stored.',)\n\n\nThe code below is used to store other data in the AWS S3 bucket. This data will be imported into the Shiny App.\n\n#Storing Other Data used by app / report in s3 bucket:\nboard = board_s3(\"devakshah-stat468-models\", allow_pickle_read=True)\nboard.pin_write(df, name=\"regression_input_data\", type=\"csv\")\n\nWriting pin:\nName: 'regression_input_data'\nVersion: 20250804T201549Z-c3a37\n\n\nMeta(title='regression_input_data: a pinned 288 x 14 DataFrame', description=None, created='20250804T201549Z', pin_hash='c3a374df234b2935', file='regression_input_data.csv', file_size=26991, type='csv', api_version=1, version=Version(created=datetime.datetime(2025, 8, 4, 20, 15, 49, 442041), hash='c3a374df234b2935'), tags=None, name='regression_input_data', user={}, local={})"
  }
]